{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea51248",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93241c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, math, random\n",
    "from dataclasses import asdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "# --- project root & local modules ---\n",
    "PROJ_ROOT = \"/home/hernan_melmoth/Documents/phd_work/otu-taxa-foundation\"\n",
    "sys.path.append(os.path.join(PROJ_ROOT, \"src\"))\n",
    "\n",
    "from otu_taxa.helpers_pretraining_model import MetricsLogger, set_seed, save_checkpoint, IGNORE_INDEX\n",
    "from otu_taxa.trainer_hier_joint_unk import run_epoch\n",
    "\n",
    "from otu_taxa.joint_hier_loss_metrics_unk import make_factorized_tax_loss_fn_fast_masked_with_unk\n",
    "from otu_taxa.otu_taxa_transformer_unk import ModelConfig, OTUTaxaTransformerEmbedTaxTreeUnkTaxa\n",
    "from otu_taxa.dataloaders_unk_balanced import (\n",
    "    OTUTaxaDataset,\n",
    "    MaskingConfig,\n",
    "    make_collator_balanced,\n",
    "    build_tax2ancestor_at_rank,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6634bcb4",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0141a9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCA distance matrix: torch.Size([6929, 6929])\n",
      "T_real (original taxa, just k UNK): 6929\n",
      "T_base (real + UNK): 6935\n",
      "Descendant matrix with UNK: torch.Size([6935, 6935])\n",
      "Descendant matrix (real only): torch.Size([6929, 6929])\n",
      "rank_idx: (6935,)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# Paths & config (single source of truth)\n",
    "# -----------------------------------\n",
    "PROJ_ROOT = \"/home/hernan_melmoth/Documents/phd_work/otu-taxa-foundation\"\n",
    "\n",
    "# External (heavy) dataset location (outside repo)\n",
    "DATASET_ROOT = \"/home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training\"\n",
    "dataset_folder_name = \"dataset_full_top999\"   # <- adjust if you used a different name\n",
    "dataset_dir = os.path.join(\n",
    "    DATASET_ROOT,\n",
    "    \"level_97\",\n",
    "    \"silva-138.2\",\n",
    "    \"incomplete_silva_sintax\",\n",
    "    dataset_folder_name,\n",
    ")\n",
    "\n",
    "# Base dataset artifacts\n",
    "TAXONOMY_VOCAB_PATH = os.path.join(dataset_dir, \"taxonomy_vocab.json\")\n",
    "path_to_taxonomy_tree = os.path.join(dataset_dir, \"taxonomy_nested.json\")\n",
    "SAMPLES_JSONL = os.path.join(dataset_dir, \"samples.jsonl\")\n",
    "\n",
    "# Tree artifacts directory (derived)\n",
    "TREE_DIR = os.path.join(dataset_dir, \"tree_artifacts\")\n",
    "\n",
    "# LCA (prefer .npy in new pipeline; keep CSV fallback if you still have it)\n",
    "LCA_NPY = os.path.join(TREE_DIR, \"lca_distance_edges.npy\")\n",
    "LCA_CSV = os.path.join(TREE_DIR, \"lca_distance_edges.csv\")  # optional legacy\n",
    "\n",
    "# Descendant matrices\n",
    "DESCENDANT_MATRIX_PATH = os.path.join(TREE_DIR, \"descendant_matrix.npy\")              # real tree only\n",
    "UNK_VOCAB_PATH         = os.path.join(TREE_DIR, \"taxonomy_vocab_with_unk.json\")       # real + 7 UNKs\n",
    "UNK_M_PATH             = os.path.join(TREE_DIR, \"descendant_matrix_with_unk.npy\")     # real+UNK closure\n",
    "RANK_IDX_PATH          = os.path.join(TREE_DIR, \"rank_idx.npy\")                       # rank per token (0..6)\n",
    "\n",
    "# # Optional: test split ids (only if you created it for the full corpus)\n",
    "# TEST_IDS_PATH = os.path.join(dataset_dir, \"splits\", \"test_samples_2000.txt\")  # adjust if needed\n",
    "\n",
    "# Run / output dir (inside repo)\n",
    "run_name = \"pretrain_hier_joint_unk_taxa\"\n",
    "out_dir = os.path.join(PROJ_ROOT, \"runs_hier_joint_unk_taxa\", run_name)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# -----------------------------------\n",
    "# Load LCA distance matrix\n",
    "# -----------------------------------\n",
    "if os.path.exists(LCA_NPY):\n",
    "    D_tree = torch.from_numpy(np.load(LCA_NPY)).float()\n",
    "else:\n",
    "    df_D = pd.read_csv(LCA_CSV, index_col=0)\n",
    "    D_tree = torch.tensor(df_D.values, dtype=torch.float32)\n",
    "\n",
    "print(\"LCA distance matrix:\", D_tree.shape)\n",
    "\n",
    "# save logs\n",
    "metrics_path = os.path.join(out_dir, \"metrics.jsonl\")\n",
    "logger = MetricsLogger(metrics_path)\n",
    "\n",
    "# -----------------------------------\n",
    "# Taxonomy sizes: T_real vs T_base\n",
    "# -----------------------------------\n",
    "# ORIGINAL vocab (no UNKs), for tree regularizer and T_real\n",
    "with open(TAXONOMY_VOCAB_PATH, \"r\") as f:\n",
    "    tax_vocab_real = json.load(f)\n",
    "T_real = len(tax_vocab_real)\n",
    "print(\"T_real (original taxa, just k UNK):\", T_real)\n",
    "\n",
    "# UNK-extended vocab & descendant matrix for hierarchical loss\n",
    "with open(UNK_VOCAB_PATH, \"r\") as f:\n",
    "    tax_vocab_unk = json.load(f)\n",
    "T_base = len(tax_vocab_unk)\n",
    "print(\"T_base (real + UNK):\", T_base)\n",
    "\n",
    "# Load UNK-extended descendant matrix (used by hierarchical loss)\n",
    "D_np_unk = np.load(UNK_M_PATH)          # [T_base, T_base]\n",
    "M_tensor = torch.from_numpy(D_np_unk)   # keep name M_tensor for the loss\n",
    "print(\"Descendant matrix with UNK:\", M_tensor.shape)\n",
    "\n",
    "# Optional: real-only descendant matrix (if needed for debugging)\n",
    "D_np_real = np.load(DESCENDANT_MATRIX_PATH)        # [T_real, T_real]\n",
    "descendant_matrix_real = torch.from_numpy(D_np_real)\n",
    "print(\"Descendant matrix (real only):\", descendant_matrix_real.shape)\n",
    "\n",
    "# Optional: rank_idx (often used by loss/metrics/collator)\n",
    "rank_idx = np.load(RANK_IDX_PATH)  # shape [T_base]\n",
    "print(\"rank_idx:\", rank_idx.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c19e1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def compute_length_percentiles(samples_jsonl_path, percentiles=(50, 75, 90, 95, 99, 99.9)):\n",
    "#     lengths = []\n",
    "#     with open(samples_jsonl_path, \"r\") as f:\n",
    "#         for line in tqdm(f, desc=\"Reading sample lengths\"):\n",
    "#             if not line.strip():\n",
    "#                 continue\n",
    "#             rec = json.loads(line)\n",
    "#             lengths.append(len(rec[\"otus\"]))\n",
    "#     lengths = np.asarray(lengths, dtype=np.int32)\n",
    "\n",
    "#     out = {p: int(np.percentile(lengths, p)) for p in percentiles}\n",
    "#     return out, lengths\n",
    "\n",
    "# pct, lengths = compute_length_percentiles(SAMPLES_JSONL, percentiles=(90, 95, 99))\n",
    "# print(\"Length percentiles:\", pct)\n",
    "# print(\"Max length:\", int(lengths.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d3c53d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Training hyperparameters\n",
    "# (experiment-specific configuration)\n",
    "# -----------------------------------\n",
    "seed = 123\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "epochs = 1\n",
    "train_batch_size = int(32 * 2)\n",
    "val_batch_size   = 64\n",
    "\n",
    "max_len = 500                    # 96th percentile sequence length\n",
    "mlm_prob = 0.15                  # masking rate\n",
    "prob_joint, prob_otu_only, prob_tax_only = 0.50, 0.25, 0.25\n",
    "keep_prob, random_prob = 0.10, 0.10\n",
    "\n",
    "\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-3\n",
    "warmup_ratio = 0.06\n",
    "max_grad_norm = 1.0\n",
    "grad_accum_steps = 2\n",
    "\n",
    "TREE_LAMBDA = 10                 # tree regularization weight\n",
    "num_workers = 0                  # 0 for determinism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4d6d9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset size: N=1836250\n",
      "[SPLIT] Train=1796250  Val=20000  Test=20000  (Total N=1836250)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# Dataset & random split (20k TEST, 10k VAL, rest TRAIN)\n",
    "# -----------------------------------\n",
    "set_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# 1) Load full dataset once\n",
    "ds = OTUTaxaDataset(dataset_dir)\n",
    "N = len(ds)\n",
    "print(f\"[INFO] Dataset size: N={N}\")\n",
    "\n",
    "# 2) Choose split sizes (cap if dataset is smaller)\n",
    "TEST_N = min(20_000, N)\n",
    "VAL_N  = min(20_000, N - TEST_N)\n",
    "\n",
    "# 3) Random permutation of indices\n",
    "all_idx = list(range(N))\n",
    "random.shuffle(all_idx)\n",
    "\n",
    "test_idx  = sorted(all_idx[:TEST_N])\n",
    "val_idx   = sorted(all_idx[TEST_N:TEST_N + VAL_N])\n",
    "train_idx = sorted(all_idx[TEST_N + VAL_N:])\n",
    "\n",
    "print(f\"[SPLIT] Train={len(train_idx)}  Val={len(val_idx)}  Test={len(test_idx)}  (Total N={N})\")\n",
    "\n",
    "# 4) Subsets\n",
    "train_ds = Subset(ds, train_idx)\n",
    "val_ds   = Subset(ds, val_idx)\n",
    "test_ds  = Subset(ds, test_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7540bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Collators & loaders\n",
    "# -----------------------------------\n",
    "\n",
    "# TRAIN collator: stochastic masking + BOTH balancing strategies\n",
    "train_cfg = MaskingConfig(\n",
    "    mlm_prob=mlm_prob,\n",
    "    prob_joint=prob_joint,\n",
    "    prob_otu_only=prob_otu_only,\n",
    "    prob_tax_only=prob_tax_only,\n",
    "    max_len=max_len,\n",
    "    keep_prob=keep_prob,\n",
    "    random_prob=random_prob,\n",
    "    balance_mode=\"otu\",\n",
    ")\n",
    "\n",
    "train_collate = make_collator_balanced(\n",
    "    dataset=ds,\n",
    "    cfg=train_cfg,\n",
    ")\n",
    "\n",
    "# VAL / TEST collator:\n",
    "# Recommended: keep masking same, but DISABLE balancing (so evaluation is not distribution-shaped).\n",
    "val_cfg = MaskingConfig(\n",
    "    mlm_prob=mlm_prob,\n",
    "    prob_joint=prob_joint,\n",
    "    prob_otu_only=prob_otu_only,\n",
    "    prob_tax_only=prob_tax_only,\n",
    "    max_len=max_len,\n",
    "    keep_prob=keep_prob,\n",
    "    random_prob=random_prob,\n",
    "    balance_mode=\"none\",    # important: no endpoint re-selection for eval\n",
    ")\n",
    "\n",
    "val_collate = make_collator_balanced(\n",
    "    dataset=ds,\n",
    "    cfg=val_cfg,\n",
    ")\n",
    "\n",
    "test_collate = make_collator_balanced(\n",
    "    dataset=ds,\n",
    "    cfg=val_cfg,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=train_collate,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=val_batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=val_collate,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=val_batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=test_collate,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "# -----------------------------------\n",
    "# Model sizes & PAD ids (single place)\n",
    "# -----------------------------------\n",
    "\n",
    "# OTUs: unchanged\n",
    "n_otus = ds.O + 2            # + pad, mask\n",
    "pad_otu_id = ds.O\n",
    "\n",
    "# TAXA (UNK-aware):\n",
    "# - T_real = original number of taxa (from taxonomy_vocab.json)\n",
    "# - T_base = len(tax_vocab_unk) = T_real + 7 (real + UNKs)\n",
    "# - n_taxa = T_base + 2 (PAD, MASK at the end)\n",
    "n_taxa = T_base + 2\n",
    "pad_tax_id  = T_base         # PAD token index\n",
    "mask_tax_id = T_base + 1     # MASK token index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908d2303",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ff4a5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK ids per rank: [6928, 6929, 6930, 6931, 6932, 6933, 6934]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hernan_melmoth/anaconda3/envs/bio_ontology_env/lib/python3.9/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Train=1796250  Val=20000  Steps/epoch=14034  Total steps=14034\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Build model config\n",
    "# -----------------------------\n",
    "model_cfg = ModelConfig(\n",
    "    d_model=256, n_layers=6, n_heads=4, d_ff=1024,\n",
    "    dropout=0.1, activation=\"gelu\",\n",
    "    tie_otu_weights=True,\n",
    "    otu_loss_weight=1.0,\n",
    "    tax_loss_weight=1.0,\n",
    "    #lambda_tree=TREE_LAMBDA,\n",
    "    emb_dropout=0.1,\n",
    "    layernorm_emb=True,\n",
    "    #lca_csv=LCA_CSV,          # LCA over ORIGINAL taxonomy (T_real x T_real)\n",
    "    T_real=T_real,            # <- NEW: number of original taxa for tree regularizer\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Hierarchical taxonomy loss (UNK-aware)\n",
    "# -----------------------------\n",
    "# Here we use the UNK-extended vocab and M_tensor that we loaded earlier:\n",
    "#   - tax_vocab_unk: list[str], len = T_base = T_real + 7\n",
    "#   - M_tensor: [T_base, T_base] descendant-closure with UNKs\n",
    "\n",
    "def build_rank_idx_from_vocab(vocab_list):\n",
    "    \"\"\"\n",
    "    Returns a LongTensor [len(vocab_list)] with rank indices 0..6 for k..s.\n",
    "    Assumes tokens start with 'k:', 'p:', ..., 's:' (including UNK ones).\n",
    "    \"\"\"\n",
    "    rank_map = {'k': 0, 'p': 1, 'c': 2, 'o': 3, 'f': 4, 'g': 5, 's': 6}\n",
    "    out = []\n",
    "    for name in vocab_list:\n",
    "        ch = name[0].lower()\n",
    "        out.append(rank_map.get(ch, -1))\n",
    "    return torch.tensor(out, dtype=torch.long)\n",
    "\n",
    "rank_idx = build_rank_idx_from_vocab(tax_vocab_unk)   # [T_base]\n",
    "\n",
    "# build the UNK-aware hierarchical loss callable\n",
    "hier_tax_loss_fn = make_factorized_tax_loss_fn_fast_masked_with_unk(\n",
    "    M_tensor=M_tensor,          # [T_base, T_base] with UNKs\n",
    "    rank_idx=rank_idx,          # [T_base]\n",
    "    tax_vocab=tax_vocab_unk,    # len = T_base\n",
    "    T_base=T_base,              # real + 7 UNKs\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Init model (UNK-aware)\n",
    "# -----------------------------\n",
    "model = OTUTaxaTransformerEmbedTaxTreeUnkTaxa(\n",
    "    n_otus=n_otus,\n",
    "    n_taxa=n_taxa,              # = T_base + 2 (PAD, MASK)\n",
    "    pad_otu_id=pad_otu_id,\n",
    "    pad_tax_id=pad_tax_id,\n",
    "    config=model_cfg,\n",
    "    tax_loss_fn=hier_tax_loss_fn,\n",
    ").to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Optimizer & Scheduler\n",
    "# -----------------------------\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "steps_per_epoch = math.ceil(len(train_loader) / max(1, grad_accum_steps))\n",
    "total_steps = steps_per_epoch * epochs\n",
    "warmup_steps = max(1, int(warmup_ratio * total_steps))\n",
    "\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    return max(0.0, float(total_steps - current_step) / float(max(1, total_steps - warmup_steps)))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# AMP scaler\n",
    "scaler = GradScaler(enabled=(device == \"cuda\"))\n",
    "\n",
    "# -----------------------------\n",
    "# Save run config\n",
    "# -----------------------------\n",
    "run_meta = {\n",
    "    \"seed\": seed,\n",
    "    \"epochs\": epochs,\n",
    "    \"train_batch_size\": train_batch_size,\n",
    "    \"val_batch_size\": val_batch_size,\n",
    "    \"max_len\": max_len,\n",
    "    \"mlm_prob\": mlm_prob,\n",
    "    \"prob_split\": [prob_joint, prob_otu_only, prob_tax_only],\n",
    "    \"optimizer\": {\"lr\": lr, \"weight_decay\": weight_decay},\n",
    "    \"sched\": {\n",
    "        \"warmup_ratio\": warmup_ratio,\n",
    "        \"total_steps\": total_steps,\n",
    "        \"warmup_steps\": warmup_steps,\n",
    "    },\n",
    "    \"model_cfg\": asdict(model_cfg),\n",
    "    \"dataset\": {\n",
    "        \"N\": N,\n",
    "        \"N_train\": len(train_ds),\n",
    "        \"N_val\": len(val_ds),\n",
    "        \"O\": ds.O,\n",
    "        # Keep both for clarity\n",
    "        \"T_real\": T_real,       # original taxa (no UNK)\n",
    "        \"T_base\": T_base,       # real + UNK (used by loss)\n",
    "    },\n",
    "}\n",
    "with open(os.path.join(out_dir, \"meta.json\"), \"w\") as f:\n",
    "    json.dump(run_meta, f, indent=2)\n",
    "\n",
    "print(\n",
    "    f\"[INFO] Train={len(train_ds)}  Val={len(val_ds)}  \"\n",
    "    f\"Steps/epoch={steps_per_epoch}  Total steps={total_steps}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2078ef59",
   "metadata": {},
   "source": [
    "# training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7144d1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK IDs per rank: {0: 6928, 1: 6929, 2: 6930, 3: 6931, 4: 6932, 5: 6933, 6: 6934}\n",
      "T_base: 6935\n"
     ]
    }
   ],
   "source": [
    "rank_map = {'k':0, 'p':1, 'c':2, 'o':3, 'f':4, 'g':5, 's':6}\n",
    "unk_ids_by_rank = {}\n",
    "\n",
    "for idx, token in enumerate(tax_vocab_unk):\n",
    "    # Expect tokens like \"k:UNK\", \"p:UNK\", ..., \"s:UNK\"\n",
    "    # Adjust this condition if your naming is slightly different.\n",
    "    if token.endswith(\"UNK\") or token.endswith(\"unk\"):\n",
    "        prefix = token.split(\":\")[0]  # 'k','p',...,'s'\n",
    "        if prefix in rank_map:\n",
    "            r = rank_map[prefix]\n",
    "            unk_ids_by_rank[r] = idx\n",
    "\n",
    "print(\"UNK IDs per rank:\", unk_ids_by_rank)\n",
    "print(\"T_base:\", T_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1553ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OTUTaxaTransformerEmbedTaxTree] using otu_loss_fn=default_otu_loss_fn (id=125379798745872) kwargs=['attention_mask', 'ignore_index']\n",
      "[OTUTaxaTransformerEmbedTaxTree] using tax_loss_fn=loss_fn (id=125379841338384) kwargs=['ignore_index']\n",
      "[OTUTaxaTransformerEmbedTaxTree] using combine_loss_fn=default_combine_loss_fn (id=125379798745584) kwargs=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hernan_melmoth/anaconda3/envs/bio_ontology_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E01] TRAIN global_step=50 batch=100\n",
      "[E01] TRAIN global_step=100 batch=200\n",
      "[E01] TRAIN global_step=150 batch=300\n",
      "[E01] TRAIN global_step=200 batch=400\n",
      "[E01] TRAIN global_step=250 batch=500\n",
      "[E01] TRAIN global_step=300 batch=600\n",
      "[E01] TRAIN global_step=350 batch=700\n",
      "[E01] TRAIN global_step=400 batch=800\n",
      "[E01] TRAIN global_step=450 batch=900\n",
      "[E01] TRAIN global_step=500 batch=1000\n",
      "[E01] TRAIN global_step=550 batch=1100\n",
      "[E01] TRAIN global_step=600 batch=1200\n",
      "[E01] TRAIN global_step=650 batch=1300\n",
      "[E01] TRAIN global_step=700 batch=1400\n",
      "[E01] TRAIN global_step=750 batch=1500\n",
      "[E01] TRAIN global_step=800 batch=1600\n",
      "[E01] TRAIN global_step=850 batch=1700\n",
      "[E01] TRAIN global_step=900 batch=1800\n",
      "[E01] TRAIN global_step=950 batch=1900\n",
      "[E01] TRAIN global_step=1000 batch=2000\n",
      "[E01] TRAIN global_step=1050 batch=2100\n",
      "[E01] TRAIN global_step=1100 batch=2200\n",
      "[E01] TRAIN global_step=1150 batch=2300\n",
      "[E01] TRAIN global_step=1200 batch=2400\n",
      "[E01] TRAIN global_step=1250 batch=2500\n",
      "[E01] TRAIN global_step=1300 batch=2600\n",
      "[E01] TRAIN global_step=1350 batch=2700\n",
      "[E01] TRAIN global_step=1400 batch=2800\n",
      "[E01] TRAIN global_step=1450 batch=2900\n",
      "[E01] TRAIN global_step=1500 batch=3000\n",
      "[E01] TRAIN global_step=1550 batch=3100\n",
      "[E01] TRAIN global_step=1600 batch=3200\n",
      "[E01] TRAIN global_step=1650 batch=3300\n",
      "[E01] TRAIN global_step=1700 batch=3400\n",
      "[E01] TRAIN global_step=1750 batch=3500\n",
      "[E01] TRAIN global_step=1800 batch=3600\n",
      "[E01] TRAIN global_step=1850 batch=3700\n",
      "[E01] TRAIN global_step=1900 batch=3800\n",
      "[E01] TRAIN global_step=1950 batch=3900\n",
      "[E01] TRAIN global_step=2000 batch=4000\n",
      "[E01] TRAIN global_step=2050 batch=4100\n",
      "[E01] TRAIN global_step=2100 batch=4200\n",
      "[E01] TRAIN global_step=2150 batch=4300\n",
      "[E01] TRAIN global_step=2200 batch=4400\n",
      "[E01] TRAIN global_step=2250 batch=4500\n",
      "[E01] TRAIN global_step=2300 batch=4600\n",
      "[E01] TRAIN global_step=2350 batch=4700\n",
      "[E01] TRAIN global_step=2400 batch=4800\n",
      "[E01] TRAIN global_step=2450 batch=4900\n",
      "[E01] TRAIN global_step=2500 batch=5000\n",
      "[E01] TRAIN global_step=2550 batch=5100\n",
      "[E01] TRAIN global_step=2600 batch=5200\n",
      "[E01] TRAIN global_step=2650 batch=5300\n",
      "[E01] TRAIN global_step=2700 batch=5400\n",
      "[E01] TRAIN global_step=2750 batch=5500\n",
      "[E01] TRAIN global_step=2800 batch=5600\n",
      "[E01] TRAIN global_step=2850 batch=5700\n",
      "[E01] TRAIN global_step=2900 batch=5800\n",
      "[E01] TRAIN global_step=2950 batch=5900\n",
      "[E01] TRAIN global_step=3000 batch=6000\n",
      "[E01] TRAIN global_step=3050 batch=6100\n",
      "[E01] TRAIN global_step=3100 batch=6200\n",
      "[E01] TRAIN global_step=3150 batch=6300\n",
      "[E01] TRAIN global_step=3200 batch=6400\n",
      "[E01] TRAIN global_step=3250 batch=6500\n",
      "[E01] TRAIN global_step=3300 batch=6600\n",
      "[E01] TRAIN global_step=3350 batch=6700\n",
      "[E01] TRAIN global_step=3400 batch=6800\n",
      "[E01] TRAIN global_step=3450 batch=6900\n",
      "[E01] TRAIN global_step=3500 batch=7000\n",
      "[E01] TRAIN global_step=3550 batch=7100\n",
      "[E01] TRAIN global_step=3600 batch=7200\n",
      "[E01] TRAIN global_step=3650 batch=7300\n",
      "[E01] TRAIN global_step=3700 batch=7400\n",
      "[E01] TRAIN global_step=3750 batch=7500\n",
      "[E01] TRAIN global_step=3800 batch=7600\n",
      "[E01] TRAIN global_step=3850 batch=7700\n",
      "[E01] TRAIN global_step=3900 batch=7800\n",
      "[E01] TRAIN global_step=3950 batch=7900\n"
     ]
    }
   ],
   "source": [
    "best_val = float(\"inf\")\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    # ---- TRAIN ----\n",
    "    train_stats, global_step = run_epoch(\n",
    "        model=model,\n",
    "        dataloader=train_loader,\n",
    "        device=device,\n",
    "        IGNORE_INDEX=IGNORE_INDEX,\n",
    "        split=\"train\",\n",
    "        epoch=epoch,\n",
    "        global_step=global_step,\n",
    "        M_tensor=M_tensor,\n",
    "        rank_idx=rank_idx,\n",
    "        T_base=T_base,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        scaler=scaler,\n",
    "        grad_accum_steps=grad_accum_steps,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        logger=logger,\n",
    "        deterministic_masks=False,\n",
    "\n",
    "        # NEW controls (UPDATED)\n",
    "        metric_every_steps=1000,            # log train_step every 1000 optimizer steps\n",
    "        train_metric_max_positions=2000,    # cap metric compute on probe batches\n",
    "        progress_every_steps=50,\n",
    "    )\n",
    "\n",
    "    # ---- VALIDATION ----\n",
    "    val_stats, _ = run_epoch(\n",
    "        model=model,\n",
    "        dataloader=val_loader,\n",
    "        device=device,\n",
    "        IGNORE_INDEX=IGNORE_INDEX,\n",
    "        split=\"val\",\n",
    "        epoch=epoch,\n",
    "        global_step=global_step,\n",
    "        M_tensor=M_tensor,\n",
    "        rank_idx=rank_idx,\n",
    "        T_base=T_base,\n",
    "        optimizer=None,\n",
    "        scheduler=None,\n",
    "        scaler=None,\n",
    "        grad_accum_steps=1,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        logger=logger,\n",
    "        deterministic_masks=True,\n",
    "\n",
    "        # OPTIONAL: evaluate only a subset if val is huge\n",
    "        # max_eval_batches=200,   # uncomment if needed\n",
    "\n",
    "        progress_every_steps=0,\n",
    "    )\n",
    "\n",
    "    # save best checkpoint (still using val loss as criterion)\n",
    "    if val_stats[\"loss\"] < best_val:\n",
    "        best_val = val_stats[\"loss\"]\n",
    "        save_checkpoint(\n",
    "            os.path.join(out_dir, \"best.pt\"),\n",
    "            model, optimizer, scheduler, scaler,\n",
    "            epoch, global_step, best_val,\n",
    "        )\n",
    "        print(f\"[E{epoch:02d}] âœ… Saved BEST\")\n",
    "\n",
    "# FINAL CHECKPOINT\n",
    "save_checkpoint(\n",
    "    os.path.join(out_dir, \"last.pt\"),\n",
    "    model, optimizer, scheduler, scaler,\n",
    "    epoch, global_step, best_val,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1981416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "# ======== CONFIG ========\n",
    "metrics_path = os.path.join(out_dir, \"metrics.jsonl\")  # your run folder\n",
    "# =========================\n",
    "\n",
    "# --- load and tidy ---\n",
    "rows = [json.loads(l) for l in open(metrics_path) if l.strip()]\n",
    "df = pd.DataFrame(rows).sort_values([\"epoch\", \"step\"], ignore_index=True)\n",
    "\n",
    "train_ep = df[df[\"split\"] == \"train_epoch\"].copy()\n",
    "val_ep   = df[df[\"split\"] == \"val_epoch\"].copy()\n",
    "\n",
    "# --- colors ---\n",
    "c_total, c_otu, c_tax, c_tree = \"C3\", \"C0\", \"C1\", \"C2\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# # --- total loss ---\n",
    "# if \"loss\" in train_ep.columns and \"loss\" in val_ep.columns:\n",
    "#     ax.plot(train_ep[\"epoch\"], train_ep[\"loss\"], \"o-\",  color=c_total, label=\"train total\")\n",
    "#     ax.plot(val_ep[\"epoch\"],   val_ep[\"loss\"],   \"o--\", color=c_total, label=\"val total\")\n",
    "\n",
    "# --- OTU / TAX losses ---\n",
    "if {\"loss_otu\",\"loss_tax\"}.issubset(train_ep.columns) and {\"loss_otu\",\"loss_tax\"}.issubset(val_ep.columns):\n",
    "    ax.plot(train_ep[\"epoch\"], train_ep[\"loss_otu\"], \"s-\",  color=c_otu, label=\"train OTU\")\n",
    "    ax.plot(val_ep[\"epoch\"],   val_ep[\"loss_otu\"],   \"s--\", color=c_otu, label=\"val OTU\")\n",
    "    ax.plot(train_ep[\"epoch\"], train_ep[\"loss_tax\"], \"^-\",  color=c_tax, label=\"train TAX\")\n",
    "    ax.plot(val_ep[\"epoch\"],   val_ep[\"loss_tax\"],   \"^--\", color=c_tax, label=\"val TAX\")\n",
    "\n",
    "# --- Tree loss (optional) ---\n",
    "if \"loss_tree\" in train_ep.columns and \"loss_tree\" in val_ep.columns:\n",
    "    ax.plot(train_ep[\"epoch\"], 10*train_ep[\"loss_tree\"], \"d-\",  color=c_tree, label=\"train TREE\")\n",
    "    ax.plot(val_ep[\"epoch\"],   10*val_ep[\"loss_tree\"],   \"d--\", color=c_tree, label=\"val TREE\")\n",
    "\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_title(\"Loss components per epoch\")\n",
    "ax.grid(alpha=0.25)\n",
    "ax.legend(ncol=4, loc=\"upper right\", frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c3aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_st = df[df[\"split\"] == \"train_step\"].copy()\n",
    "val_st   = df[df[\"split\"] == \"val_step\"].copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "if \"loss\" in train_st.columns:\n",
    "    ax.plot(train_st[\"step\"], train_st[\"loss\"], \"-\", label=\"train total (window)\")\n",
    "if \"loss\" in val_st.columns:\n",
    "    ax.plot(val_st[\"step\"], val_st[\"loss\"], \"--\", label=\"val total (window)\")\n",
    "\n",
    "ax.set_xlabel(\"global step\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_title(\"Loss (windowed logs)\")\n",
    "ax.grid(alpha=0.25)\n",
    "ax.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_ontology_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

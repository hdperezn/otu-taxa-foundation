{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea51248",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93241c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, math, random\n",
    "from dataclasses import asdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "# --- project root & local modules ---\n",
    "PROJ_ROOT = \"/home/hernan_melmoth/Documents/phd_work/otu-taxa-foundation\"\n",
    "sys.path.append(os.path.join(PROJ_ROOT, \"src\"))\n",
    "\n",
    "from otu_taxa.helpers_pretraining_model import MetricsLogger, set_seed, save_checkpoint, IGNORE_INDEX\n",
    "from otu_taxa.trainer_hier_joint_unk import run_epoch\n",
    "\n",
    "from otu_taxa.joint_hier_loss_metrics_unk import make_factorized_tax_loss_fn_fast_masked_with_unk\n",
    "from otu_taxa.otu_taxa_transformer_unk import ModelConfig, OTUTaxaTransformerEmbedTaxTreeUnkTaxa\n",
    "from otu_taxa.dataloaders_unk_balanced import (\n",
    "    OTUTaxaDataset,\n",
    "    MaskingConfig,\n",
    "    make_collator_balanced,\n",
    "    build_tax2ancestor_at_rank,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6634bcb4",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0141a9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCA distance matrix: torch.Size([6929, 6929])\n",
      "T_real (original taxa, just k UNK): 6929\n",
      "T_base (real + UNK): 6935\n",
      "Descendant matrix with UNK: torch.Size([6935, 6935])\n",
      "Descendant matrix (real only): torch.Size([6929, 6929])\n",
      "rank_idx: (6935,)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# Paths & config (single source of truth)\n",
    "# -----------------------------------\n",
    "PROJ_ROOT = \"/home/hernan_melmoth/Documents/phd_work/otu-taxa-foundation\"\n",
    "\n",
    "# External (heavy) dataset location (outside repo)\n",
    "DATASET_ROOT = \"/home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training\"\n",
    "dataset_folder_name = \"dataset_full_top999\"   # <- adjust if you used a different name\n",
    "dataset_dir = os.path.join(\n",
    "    DATASET_ROOT,\n",
    "    \"level_97\",\n",
    "    \"silva-138.2\",\n",
    "    \"incomplete_silva_sintax\",\n",
    "    dataset_folder_name,\n",
    ")\n",
    "\n",
    "# Base dataset artifacts\n",
    "TAXONOMY_VOCAB_PATH = os.path.join(dataset_dir, \"taxonomy_vocab.json\")\n",
    "path_to_taxonomy_tree = os.path.join(dataset_dir, \"taxonomy_nested.json\")\n",
    "SAMPLES_JSONL = os.path.join(dataset_dir, \"samples.jsonl\")\n",
    "\n",
    "# Tree artifacts directory (derived)\n",
    "TREE_DIR = os.path.join(dataset_dir, \"tree_artifacts\")\n",
    "\n",
    "# LCA (prefer .npy in new pipeline; keep CSV fallback if you still have it)\n",
    "LCA_NPY = os.path.join(TREE_DIR, \"lca_distance_edges.npy\")\n",
    "LCA_CSV = os.path.join(TREE_DIR, \"lca_distance_edges.csv\")  # optional legacy\n",
    "\n",
    "# Descendant matrices\n",
    "DESCENDANT_MATRIX_PATH = os.path.join(TREE_DIR, \"descendant_matrix.npy\")              # real tree only\n",
    "UNK_VOCAB_PATH         = os.path.join(TREE_DIR, \"taxonomy_vocab_with_unk.json\")       # real + 7 UNKs\n",
    "UNK_M_PATH             = os.path.join(TREE_DIR, \"descendant_matrix_with_unk.npy\")     # real+UNK closure\n",
    "RANK_IDX_PATH          = os.path.join(TREE_DIR, \"rank_idx.npy\")                       # rank per token (0..6)\n",
    "\n",
    "# # Optional: test split ids (only if you created it for the full corpus)\n",
    "# TEST_IDS_PATH = os.path.join(dataset_dir, \"splits\", \"test_samples_2000.txt\")  # adjust if needed\n",
    "\n",
    "# Run / output dir (inside repo)\n",
    "run_name = \"pretrain_hier_joint_unk_taxa\"\n",
    "out_dir = os.path.join(PROJ_ROOT, \"runs_hier_joint_unk_taxa\", run_name)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# -----------------------------------\n",
    "# Load LCA distance matrix\n",
    "# -----------------------------------\n",
    "if os.path.exists(LCA_NPY):\n",
    "    D_tree = torch.from_numpy(np.load(LCA_NPY)).float()\n",
    "else:\n",
    "    df_D = pd.read_csv(LCA_CSV, index_col=0)\n",
    "    D_tree = torch.tensor(df_D.values, dtype=torch.float32)\n",
    "\n",
    "print(\"LCA distance matrix:\", D_tree.shape)\n",
    "\n",
    "# save logs\n",
    "metrics_path = os.path.join(out_dir, \"metrics.jsonl\")\n",
    "logger = MetricsLogger(metrics_path)\n",
    "\n",
    "# -----------------------------------\n",
    "# Taxonomy sizes: T_real vs T_base\n",
    "# -----------------------------------\n",
    "# ORIGINAL vocab (no UNKs), for tree regularizer and T_real\n",
    "with open(TAXONOMY_VOCAB_PATH, \"r\") as f:\n",
    "    tax_vocab_real = json.load(f)\n",
    "T_real = len(tax_vocab_real)\n",
    "print(\"T_real (original taxa, just k UNK):\", T_real)\n",
    "\n",
    "# UNK-extended vocab & descendant matrix for hierarchical loss\n",
    "with open(UNK_VOCAB_PATH, \"r\") as f:\n",
    "    tax_vocab_unk = json.load(f)\n",
    "T_base = len(tax_vocab_unk)\n",
    "print(\"T_base (real + UNK):\", T_base)\n",
    "\n",
    "# Load UNK-extended descendant matrix (used by hierarchical loss)\n",
    "D_np_unk = np.load(UNK_M_PATH)          # [T_base, T_base]\n",
    "M_tensor = torch.from_numpy(D_np_unk)   # keep name M_tensor for the loss\n",
    "print(\"Descendant matrix with UNK:\", M_tensor.shape)\n",
    "\n",
    "# Optional: real-only descendant matrix (if needed for debugging)\n",
    "D_np_real = np.load(DESCENDANT_MATRIX_PATH)        # [T_real, T_real]\n",
    "descendant_matrix_real = torch.from_numpy(D_np_real)\n",
    "print(\"Descendant matrix (real only):\", descendant_matrix_real.shape)\n",
    "\n",
    "# Optional: rank_idx (often used by loss/metrics/collator)\n",
    "rank_idx = np.load(RANK_IDX_PATH)  # shape [T_base]\n",
    "print(\"rank_idx:\", rank_idx.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c19e1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def compute_length_percentiles(samples_jsonl_path, percentiles=(50, 75, 90, 95, 99, 99.9)):\n",
    "#     lengths = []\n",
    "#     with open(samples_jsonl_path, \"r\") as f:\n",
    "#         for line in tqdm(f, desc=\"Reading sample lengths\"):\n",
    "#             if not line.strip():\n",
    "#                 continue\n",
    "#             rec = json.loads(line)\n",
    "#             lengths.append(len(rec[\"otus\"]))\n",
    "#     lengths = np.asarray(lengths, dtype=np.int32)\n",
    "\n",
    "#     out = {p: int(np.percentile(lengths, p)) for p in percentiles}\n",
    "#     return out, lengths\n",
    "\n",
    "# pct, lengths = compute_length_percentiles(SAMPLES_JSONL, percentiles=(90, 95, 99))\n",
    "# print(\"Length percentiles:\", pct)\n",
    "# print(\"Max length:\", int(lengths.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d3c53d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Training hyperparameters\n",
    "# (experiment-specific configuration)\n",
    "# -----------------------------------\n",
    "seed = 123\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "epochs = 40\n",
    "train_batch_size = int(32 * 2)\n",
    "val_batch_size   = 64\n",
    "\n",
    "max_len = 500                    # 96th percentile sequence length\n",
    "mlm_prob = 0.15                  # masking rate\n",
    "prob_joint, prob_otu_only, prob_tax_only = 0.50, 0.25, 0.25\n",
    "keep_prob, random_prob = 0.10, 0.10\n",
    "\n",
    "\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-3\n",
    "warmup_ratio = 0.06\n",
    "max_grad_norm = 1.0\n",
    "grad_accum_steps = 2\n",
    "\n",
    "TREE_LAMBDA = 10                 # tree regularization weight\n",
    "num_workers = 0                  # 0 for determinism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4d6d9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset size: N=1836250\n",
      "[SPLIT] Train=1806250  Val=10000  Test=20000  (Total N=1836250)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# Dataset & random split (20k TEST, 10k VAL, rest TRAIN)\n",
    "# -----------------------------------\n",
    "set_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# 1) Load full dataset once\n",
    "ds = OTUTaxaDataset(dataset_dir)\n",
    "N = len(ds)\n",
    "print(f\"[INFO] Dataset size: N={N}\")\n",
    "\n",
    "# 2) Choose split sizes (cap if dataset is smaller)\n",
    "TEST_N = min(20_000, N)\n",
    "VAL_N  = min(10_000, N - TEST_N)\n",
    "\n",
    "# 3) Random permutation of indices\n",
    "all_idx = list(range(N))\n",
    "random.shuffle(all_idx)\n",
    "\n",
    "test_idx  = sorted(all_idx[:TEST_N])\n",
    "val_idx   = sorted(all_idx[TEST_N:TEST_N + VAL_N])\n",
    "train_idx = sorted(all_idx[TEST_N + VAL_N:])\n",
    "\n",
    "print(f\"[SPLIT] Train={len(train_idx)}  Val={len(val_idx)}  Test={len(test_idx)}  (Total N={N})\")\n",
    "\n",
    "# 4) Subsets\n",
    "train_ds = Subset(ds, train_idx)\n",
    "val_ds   = Subset(ds, val_idx)\n",
    "test_ds  = Subset(ds, test_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7540bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Collators & loaders\n",
    "# -----------------------------------\n",
    "\n",
    "# TRAIN collator: stochastic masking + BOTH balancing strategies\n",
    "train_cfg = MaskingConfig(\n",
    "    mlm_prob=mlm_prob,\n",
    "    prob_joint=prob_joint,\n",
    "    prob_otu_only=prob_otu_only,\n",
    "    prob_tax_only=prob_tax_only,\n",
    "    max_len=max_len,\n",
    "    keep_prob=keep_prob,\n",
    "    random_prob=random_prob,\n",
    "    balance_mode=\"otu\",\n",
    ")\n",
    "\n",
    "train_collate = make_collator_balanced(\n",
    "    dataset=ds,\n",
    "    cfg=train_cfg,\n",
    ")\n",
    "\n",
    "# VAL / TEST collator:\n",
    "# Recommended: keep masking same, but DISABLE balancing (so evaluation is not distribution-shaped).\n",
    "val_cfg = MaskingConfig(\n",
    "    mlm_prob=mlm_prob,\n",
    "    prob_joint=prob_joint,\n",
    "    prob_otu_only=prob_otu_only,\n",
    "    prob_tax_only=prob_tax_only,\n",
    "    max_len=max_len,\n",
    "    keep_prob=keep_prob,\n",
    "    random_prob=random_prob,\n",
    "    balance_mode=\"none\",    # important: no endpoint re-selection for eval\n",
    ")\n",
    "\n",
    "val_collate = make_collator_balanced(\n",
    "    dataset=ds,\n",
    "    cfg=val_cfg,\n",
    ")\n",
    "\n",
    "test_collate = make_collator_balanced(\n",
    "    dataset=ds,\n",
    "    cfg=val_cfg,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=train_collate,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=val_batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=val_collate,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=val_batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=test_collate,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "# -----------------------------------\n",
    "# Model sizes & PAD ids (single place)\n",
    "# -----------------------------------\n",
    "\n",
    "# OTUs: unchanged\n",
    "n_otus = ds.O + 2            # + pad, mask\n",
    "pad_otu_id = ds.O\n",
    "\n",
    "# TAXA (UNK-aware):\n",
    "# - T_real = original number of taxa (from taxonomy_vocab.json)\n",
    "# - T_base = len(tax_vocab_unk) = T_real + 7 (real + UNKs)\n",
    "# - n_taxa = T_base + 2 (PAD, MASK at the end)\n",
    "n_taxa = T_base + 2\n",
    "pad_tax_id  = T_base         # PAD token index\n",
    "mask_tax_id = T_base + 1     # MASK token index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908d2303",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ff4a5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK ids per rank: [6928, 6929, 6930, 6931, 6932, 6933, 6934]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hernan_melmoth/anaconda3/envs/bio_ontology_env/lib/python3.9/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Train=1806250  Val=10000  Steps/epoch=14112  Total steps=564480\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Build model config\n",
    "# -----------------------------\n",
    "model_cfg = ModelConfig(\n",
    "    d_model=256, n_layers=6, n_heads=4, d_ff=1024,\n",
    "    dropout=0.1, activation=\"gelu\",\n",
    "    tie_otu_weights=True,\n",
    "    otu_loss_weight=1.0,\n",
    "    tax_loss_weight=1.0,\n",
    "    #lambda_tree=TREE_LAMBDA,\n",
    "    emb_dropout=0.1,\n",
    "    layernorm_emb=True,\n",
    "    #lca_csv=LCA_CSV,          # LCA over ORIGINAL taxonomy (T_real x T_real)\n",
    "    T_real=T_real,            # <- NEW: number of original taxa for tree regularizer\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Hierarchical taxonomy loss (UNK-aware)\n",
    "# -----------------------------\n",
    "# Here we use the UNK-extended vocab and M_tensor that we loaded earlier:\n",
    "#   - tax_vocab_unk: list[str], len = T_base = T_real + 7\n",
    "#   - M_tensor: [T_base, T_base] descendant-closure with UNKs\n",
    "\n",
    "def build_rank_idx_from_vocab(vocab_list):\n",
    "    \"\"\"\n",
    "    Returns a LongTensor [len(vocab_list)] with rank indices 0..6 for k..s.\n",
    "    Assumes tokens start with 'k:', 'p:', ..., 's:' (including UNK ones).\n",
    "    \"\"\"\n",
    "    rank_map = {'k': 0, 'p': 1, 'c': 2, 'o': 3, 'f': 4, 'g': 5, 's': 6}\n",
    "    out = []\n",
    "    for name in vocab_list:\n",
    "        ch = name[0].lower()\n",
    "        out.append(rank_map.get(ch, -1))\n",
    "    return torch.tensor(out, dtype=torch.long)\n",
    "\n",
    "rank_idx = build_rank_idx_from_vocab(tax_vocab_unk)   # [T_base]\n",
    "\n",
    "# build the UNK-aware hierarchical loss callable\n",
    "hier_tax_loss_fn = make_factorized_tax_loss_fn_fast_masked_with_unk(\n",
    "    M_tensor=M_tensor,          # [T_base, T_base] with UNKs\n",
    "    rank_idx=rank_idx,          # [T_base]\n",
    "    tax_vocab=tax_vocab_unk,    # len = T_base\n",
    "    T_base=T_base,              # real + 7 UNKs\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Init model (UNK-aware)\n",
    "# -----------------------------\n",
    "model = OTUTaxaTransformerEmbedTaxTreeUnkTaxa(\n",
    "    n_otus=n_otus,\n",
    "    n_taxa=n_taxa,              # = T_base + 2 (PAD, MASK)\n",
    "    pad_otu_id=pad_otu_id,\n",
    "    pad_tax_id=pad_tax_id,\n",
    "    config=model_cfg,\n",
    "    tax_loss_fn=hier_tax_loss_fn,\n",
    ").to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Optimizer & Scheduler\n",
    "# -----------------------------\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "steps_per_epoch = math.ceil(len(train_loader) / max(1, grad_accum_steps))\n",
    "total_steps = steps_per_epoch * epochs\n",
    "warmup_steps = max(1, int(warmup_ratio * total_steps))\n",
    "\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    return max(0.0, float(total_steps - current_step) / float(max(1, total_steps - warmup_steps)))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# AMP scaler\n",
    "scaler = GradScaler(enabled=(device == \"cuda\"))\n",
    "\n",
    "# -----------------------------\n",
    "# Save run config\n",
    "# -----------------------------\n",
    "run_meta = {\n",
    "    \"seed\": seed,\n",
    "    \"epochs\": epochs,\n",
    "    \"train_batch_size\": train_batch_size,\n",
    "    \"val_batch_size\": val_batch_size,\n",
    "    \"max_len\": max_len,\n",
    "    \"mlm_prob\": mlm_prob,\n",
    "    \"prob_split\": [prob_joint, prob_otu_only, prob_tax_only],\n",
    "    \"optimizer\": {\"lr\": lr, \"weight_decay\": weight_decay},\n",
    "    \"sched\": {\n",
    "        \"warmup_ratio\": warmup_ratio,\n",
    "        \"total_steps\": total_steps,\n",
    "        \"warmup_steps\": warmup_steps,\n",
    "    },\n",
    "    \"model_cfg\": asdict(model_cfg),\n",
    "    \"dataset\": {\n",
    "        \"N\": N,\n",
    "        \"N_train\": len(train_ds),\n",
    "        \"N_val\": len(val_ds),\n",
    "        \"O\": ds.O,\n",
    "        # Keep both for clarity\n",
    "        \"T_real\": T_real,       # original taxa (no UNK)\n",
    "        \"T_base\": T_base,       # real + UNK (used by loss)\n",
    "    },\n",
    "}\n",
    "with open(os.path.join(out_dir, \"meta.json\"), \"w\") as f:\n",
    "    json.dump(run_meta, f, indent=2)\n",
    "\n",
    "print(\n",
    "    f\"[INFO] Train={len(train_ds)}  Val={len(val_ds)}  \"\n",
    "    f\"Steps/epoch={steps_per_epoch}  Total steps={total_steps}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2078ef59",
   "metadata": {},
   "source": [
    "# training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7144d1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK IDs per rank: {0: 6928, 1: 6929, 2: 6930, 3: 6931, 4: 6932, 5: 6933, 6: 6934}\n",
      "T_base: 6935\n"
     ]
    }
   ],
   "source": [
    "rank_map = {'k':0, 'p':1, 'c':2, 'o':3, 'f':4, 'g':5, 's':6}\n",
    "unk_ids_by_rank = {}\n",
    "\n",
    "for idx, token in enumerate(tax_vocab_unk):\n",
    "    # Expect tokens like \"k:UNK\", \"p:UNK\", ..., \"s:UNK\"\n",
    "    # Adjust this condition if your naming is slightly different.\n",
    "    if token.endswith(\"UNK\") or token.endswith(\"unk\"):\n",
    "        prefix = token.split(\":\")[0]  # 'k','p',...,'s'\n",
    "        if prefix in rank_map:\n",
    "            r = rank_map[prefix]\n",
    "            unk_ids_by_rank[r] = idx\n",
    "\n",
    "print(\"UNK IDs per rank:\", unk_ids_by_rank)\n",
    "print(\"T_base:\", T_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1553ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OTUTaxaTransformerEmbedTaxTree] using otu_loss_fn=default_otu_loss_fn (id=128442635710320) kwargs=['attention_mask', 'ignore_index']\n",
      "[OTUTaxaTransformerEmbedTaxTree] using tax_loss_fn=loss_fn (id=128442636151104) kwargs=['ignore_index']\n",
      "[OTUTaxaTransformerEmbedTaxTree] using combine_loss_fn=default_combine_loss_fn (id=128442635710032) kwargs=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hernan_melmoth/anaconda3/envs/bio_ontology_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E01] TRAIN progress: batch 500  global_step=250\n",
      "[E01] TRAIN progress: batch 1000  global_step=500\n",
      "[E01] TRAIN progress: batch 1500  global_step=750\n",
      "[E01] TRAIN step=1000 probe_acc_deep=0.108 probe_f1_deep=0.055 (used_pos=2772)\n",
      "[E01] TRAIN progress: batch 2000  global_step=1000\n",
      "[E01] TRAIN progress: batch 2500  global_step=1250\n",
      "[E01] TRAIN progress: batch 3000  global_step=1500\n",
      "[E01] TRAIN progress: batch 3500  global_step=1750\n",
      "[E01] TRAIN step=2000 probe_acc_deep=0.235 probe_f1_deep=0.148 (used_pos=2812)\n",
      "[E01] TRAIN progress: batch 4000  global_step=2000\n",
      "[E01] TRAIN progress: batch 4500  global_step=2250\n",
      "[E01] TRAIN progress: batch 5000  global_step=2500\n",
      "[E01] TRAIN progress: batch 5500  global_step=2750\n",
      "[E01] TRAIN step=3000 probe_acc_deep=0.384 probe_f1_deep=0.275 (used_pos=2998)\n",
      "[E01] TRAIN progress: batch 6000  global_step=3000\n",
      "[E01] TRAIN progress: batch 6500  global_step=3250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m global_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# ---- TRAIN ----\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     train_stats, global_step \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIGNORE_INDEX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIGNORE_INDEX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mM_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mM_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# UNK-extended [T_base, T_base]\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrank_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# built from tax_vocab_unk\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mT_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# <- IMPORTANT: UNK-aware T_base, not ds.T\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43munk_ids_by_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_ids_by_rank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_accum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_accum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeterministic_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_train_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# ---- VALIDATION ----\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     val_stats, _ \u001b[38;5;241m=\u001b[39m run_epoch(\n\u001b[1;32m     32\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     33\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m         deterministic_masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     50\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/phd_work/otu-taxa-foundation/src/otu_taxa/trainer_hier_joint_unk.py:584\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(model, dataloader, device, IGNORE_INDEX, split, epoch, global_step, M_tensor, rank_idx, T_base, unk_ids_by_rank, optimizer, scheduler, scaler, grad_accum_steps, max_grad_norm, logger, log_every, deterministic_masks, compute_train_metrics, steps_per_epoch, progress_every_batches, metrics_every_steps, max_train_metric_positions)\u001b[0m\n\u001b[1;32m    581\u001b[0m         random.seed(123)\n\u001b[1;32m    582\u001b[0m     torch.set_grad_enabled(False)\n\u001b[0;32m--> 584\u001b[0m # ---- accumulators for losses ----\n\u001b[1;32m    585\u001b[0m tot_loss = tot_loss_otu = tot_loss_tax = tot_loss_tree = 0.0\n\u001b[1;32m    586\u001b[0m seen_batches = 0\n",
      "File \u001b[0;32m~/anaconda3/envs/bio_ontology_env/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bio_ontology_env/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/bio_ontology_env/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val = float(\"inf\")\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    # ---- TRAIN ----\n",
    "    train_stats, global_step = run_epoch(\n",
    "        model=model,\n",
    "        dataloader=train_loader,\n",
    "        device=device,\n",
    "        IGNORE_INDEX=IGNORE_INDEX,\n",
    "        split=\"train\",\n",
    "        epoch=epoch,\n",
    "        global_step=global_step,\n",
    "        M_tensor=M_tensor,          # UNK-extended [T_base, T_base]\n",
    "        rank_idx=rank_idx,          # built from tax_vocab_unk\n",
    "        T_base=T_base,              # <- IMPORTANT: UNK-aware T_base, not ds.T\n",
    "        unk_ids_by_rank=unk_ids_by_rank, \n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        scaler=scaler,\n",
    "        grad_accum_steps=grad_accum_steps,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        logger=logger,\n",
    "        log_every=100,\n",
    "        deterministic_masks=False,\n",
    "        compute_train_metrics=True,\n",
    "    )\n",
    "\n",
    "    # ---- VALIDATION ----\n",
    "    val_stats, _ = run_epoch(\n",
    "        model=model,\n",
    "        dataloader=val_loader,\n",
    "        device=device,\n",
    "        IGNORE_INDEX=IGNORE_INDEX,\n",
    "        split=\"val\",\n",
    "        epoch=epoch,\n",
    "        global_step=global_step,\n",
    "        M_tensor=M_tensor,\n",
    "        rank_idx=rank_idx,\n",
    "        T_base=T_base,              # <- same T_base\n",
    "        unk_ids_by_rank=unk_ids_by_rank,\n",
    "        optimizer=None,\n",
    "        scheduler=None,\n",
    "        scaler=None,\n",
    "        grad_accum_steps=1,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        logger=logger,\n",
    "        deterministic_masks=True,\n",
    "    )\n",
    "\n",
    "    # save best checkpoint (still using val loss as criterion)\n",
    "    if val_stats[\"loss\"] < best_val:\n",
    "        best_val = val_stats[\"loss\"]\n",
    "        save_checkpoint(\n",
    "            os.path.join(out_dir, \"best.pt\"),\n",
    "            model, optimizer, scheduler, scaler,\n",
    "            epoch, global_step, best_val,\n",
    "        )\n",
    "        print(f\"[E{epoch:02d}] âœ… Saved BEST\")\n",
    "\n",
    "    # ---- TEST ----\n",
    "    test_stats, _ = run_epoch(\n",
    "        model=model,\n",
    "        dataloader=test_loader,\n",
    "        device=device,\n",
    "        IGNORE_INDEX=IGNORE_INDEX,\n",
    "        split=\"test\",\n",
    "        epoch=epoch,\n",
    "        global_step=global_step,\n",
    "        M_tensor=M_tensor,\n",
    "        rank_idx=rank_idx,\n",
    "        T_base=T_base,              # <- same T_base\n",
    "        unk_ids_by_rank=unk_ids_by_rank,\n",
    "        optimizer=None,\n",
    "        scheduler=None,\n",
    "        scaler=None,\n",
    "        grad_accum_steps=1,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        logger=logger,\n",
    "        deterministic_masks=True,\n",
    "    )\n",
    "\n",
    "# FINAL CHECKPOINT\n",
    "save_checkpoint(\n",
    "    os.path.join(out_dir, \"last.pt\"),\n",
    "    model, optimizer, scheduler, scaler,\n",
    "    epoch, global_step, best_val,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c3aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_ontology_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea51248",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93241c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, math, random\n",
    "from dataclasses import asdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "# --- project root & local modules ---\n",
    "PROJ_ROOT = \"/home/hernan_melmoth/Documents/phd_work/otu-taxa-foundation\"\n",
    "sys.path.append(os.path.join(PROJ_ROOT, \"src/otu_taxa\"))\n",
    "\n",
    "from otu_taxa.helpers_pretraining_model import MetricsLogger, set_seed, save_checkpoint, IGNORE_INDEX\n",
    "from otu_taxa.trainer_hier_joint_unk import run_epoch\n",
    "\n",
    "from otu_taxa.joint_hier_loss_metrics_unk import make_factorized_tax_loss_fn_fast_masked_with_unk\n",
    "from otu_taxa.otu_taxa_transformer_unk import ModelConfig, OTUTaxaTransformerEmbedTaxTreeUnkTaxa\n",
    "from otu_taxa.dataloaders_unk_balanced import (\n",
    "    OTUTaxaDataset,\n",
    "    MaskingConfig,\n",
    "    make_collator_balanced,\n",
    "    build_tax2ancestor_at_rank,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6634bcb4",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0141a9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCA distance matrix: torch.Size([6928, 6928])\n",
      "T_real (original taxa, no UNK): 6928\n",
      "T_base (real + UNK): 6935\n",
      "Descendant matrix with UNK: torch.Size([6935, 6935])\n",
      "Descendant matrix (real only): torch.Size([6928, 6928])\n",
      "rank_idx: (6935,)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# Paths & config (single source of truth)\n",
    "# -----------------------------------\n",
    "PROJ_ROOT = \"/home/hernan_melmoth/Documents/phd_work/otu-taxa-foundation\"\n",
    "\n",
    "# External (heavy) dataset location (outside repo)\n",
    "DATASET_ROOT = \"/home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training\"\n",
    "dataset_folder_name = \"dataset_full_top999\"   # <- adjust if you used a different name\n",
    "dataset_dir = os.path.join(\n",
    "    DATASET_ROOT,\n",
    "    \"level_97\",\n",
    "    \"silva-138.2\",\n",
    "    \"incomplete_silva_sintax\",\n",
    "    dataset_folder_name,\n",
    ")\n",
    "\n",
    "# Base dataset artifacts\n",
    "TAXONOMY_VOCAB_PATH = os.path.join(dataset_dir, \"taxonomy_vocab.json\")\n",
    "path_to_taxonomy_tree = os.path.join(dataset_dir, \"taxonomy_nested.json\")\n",
    "SAMPLES_JSONL = os.path.join(dataset_dir, \"samples.jsonl\")\n",
    "\n",
    "# Tree artifacts directory (derived)\n",
    "TREE_DIR = os.path.join(dataset_dir, \"tree_artifacts\")\n",
    "\n",
    "# LCA (prefer .npy in new pipeline; keep CSV fallback if you still have it)\n",
    "LCA_NPY = os.path.join(TREE_DIR, \"lca_distance_edges.npy\")\n",
    "LCA_CSV = os.path.join(TREE_DIR, \"lca_distance_edges.csv\")  # optional legacy\n",
    "\n",
    "# Descendant matrices\n",
    "DESCENDANT_MATRIX_PATH = os.path.join(TREE_DIR, \"descendant_matrix.npy\")              # real tree only\n",
    "UNK_VOCAB_PATH         = os.path.join(TREE_DIR, \"taxonomy_vocab_with_unk.json\")       # real + 7 UNKs\n",
    "UNK_M_PATH             = os.path.join(TREE_DIR, \"descendant_matrix_with_unk.npy\")     # real+UNK closure\n",
    "RANK_IDX_PATH          = os.path.join(TREE_DIR, \"rank_idx.npy\")                       # rank per token (0..6)\n",
    "\n",
    "# # Optional: test split ids (only if you created it for the full corpus)\n",
    "# TEST_IDS_PATH = os.path.join(dataset_dir, \"splits\", \"test_samples_2000.txt\")  # adjust if needed\n",
    "\n",
    "# Run / output dir (inside repo)\n",
    "run_name = \"pretrain_hier_joint_unk_taxa\"\n",
    "out_dir = os.path.join(PROJ_ROOT, \"runs_hier_joint_unk_taxa\", run_name)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# -----------------------------------\n",
    "# Load LCA distance matrix\n",
    "# -----------------------------------\n",
    "if os.path.exists(LCA_NPY):\n",
    "    D_tree = torch.from_numpy(np.load(LCA_NPY)).float()\n",
    "else:\n",
    "    df_D = pd.read_csv(LCA_CSV, index_col=0)\n",
    "    D_tree = torch.tensor(df_D.values, dtype=torch.float32)\n",
    "\n",
    "print(\"LCA distance matrix:\", D_tree.shape)\n",
    "\n",
    "# save logs\n",
    "metrics_path = os.path.join(out_dir, \"metrics.jsonl\")\n",
    "logger = MetricsLogger(metrics_path)\n",
    "\n",
    "# -----------------------------------\n",
    "# Taxonomy sizes: T_real vs T_base\n",
    "# -----------------------------------\n",
    "# ORIGINAL vocab (no UNKs), for tree regularizer and T_real\n",
    "with open(TAXONOMY_VOCAB_PATH, \"r\") as f:\n",
    "    tax_vocab_real = json.load(f)\n",
    "T_real = len(tax_vocab_real)\n",
    "print(\"T_real (original taxa, no UNK):\", T_real)\n",
    "\n",
    "# UNK-extended vocab & descendant matrix for hierarchical loss\n",
    "with open(UNK_VOCAB_PATH, \"r\") as f:\n",
    "    tax_vocab_unk = json.load(f)\n",
    "T_base = len(tax_vocab_unk)\n",
    "print(\"T_base (real + UNK):\", T_base)\n",
    "\n",
    "# Load UNK-extended descendant matrix (used by hierarchical loss)\n",
    "D_np_unk = np.load(UNK_M_PATH)          # [T_base, T_base]\n",
    "M_tensor = torch.from_numpy(D_np_unk)   # keep name M_tensor for the loss\n",
    "print(\"Descendant matrix with UNK:\", M_tensor.shape)\n",
    "\n",
    "# Optional: real-only descendant matrix (if needed for debugging)\n",
    "D_np_real = np.load(DESCENDANT_MATRIX_PATH)        # [T_real, T_real]\n",
    "descendant_matrix_real = torch.from_numpy(D_np_real)\n",
    "print(\"Descendant matrix (real only):\", descendant_matrix_real.shape)\n",
    "\n",
    "# Optional: rank_idx (often used by loss/metrics/collator)\n",
    "rank_idx = np.load(RANK_IDX_PATH)  # shape [T_base]\n",
    "print(\"rank_idx:\", rank_idx.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19e1d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading sample lengths: 1836250it [00:28, 64048.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length percentiles: {90: 339, 95: 477, 99: 861}\n",
      "Max length: 12546\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def compute_length_percentiles(samples_jsonl_path, percentiles=(50, 75, 90, 95, 99, 99.9)):\n",
    "#     lengths = []\n",
    "#     with open(samples_jsonl_path, \"r\") as f:\n",
    "#         for line in tqdm(f, desc=\"Reading sample lengths\"):\n",
    "#             if not line.strip():\n",
    "#                 continue\n",
    "#             rec = json.loads(line)\n",
    "#             lengths.append(len(rec[\"otus\"]))\n",
    "#     lengths = np.asarray(lengths, dtype=np.int32)\n",
    "\n",
    "#     out = {p: int(np.percentile(lengths, p)) for p in percentiles}\n",
    "#     return out, lengths\n",
    "\n",
    "# pct, lengths = compute_length_percentiles(SAMPLES_JSONL, percentiles=(90, 95, 99))\n",
    "# print(\"Length percentiles:\", pct)\n",
    "# print(\"Max length:\", int(lengths.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d3c53d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Training hyperparameters\n",
    "# (experiment-specific configuration)\n",
    "# -----------------------------------\n",
    "seed = 123\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "epochs = 40\n",
    "train_batch_size = int(32 * 8)\n",
    "val_batch_size   = 64\n",
    "\n",
    "max_len = 500                    # 96th percentile sequence length\n",
    "mlm_prob = 0.15                  # masking rate\n",
    "prob_joint, prob_otu_only, prob_tax_only = 0.50, 0.25, 0.25\n",
    "keep_prob, random_prob = 0.10, 0.10\n",
    "\n",
    "\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-3\n",
    "warmup_ratio = 0.06\n",
    "max_grad_norm = 1.0\n",
    "grad_accum_steps = 2\n",
    "\n",
    "TREE_LAMBDA = 10                 # tree regularization weight\n",
    "num_workers = 0                  # 0 for determinism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4d6d9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset size: N=1836250\n",
      "[SPLIT] Train=1806250  Val=10000  Test=20000  (Total N=1836250)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# Dataset & random split (20k TEST, 10k VAL, rest TRAIN)\n",
    "# -----------------------------------\n",
    "set_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# 1) Load full dataset once\n",
    "ds = OTUTaxaDataset(dataset_dir)\n",
    "N = len(ds)\n",
    "print(f\"[INFO] Dataset size: N={N}\")\n",
    "\n",
    "# 2) Choose split sizes (cap if dataset is smaller)\n",
    "TEST_N = min(20_000, N)\n",
    "VAL_N  = min(10_000, N - TEST_N)\n",
    "\n",
    "# 3) Random permutation of indices\n",
    "all_idx = list(range(N))\n",
    "random.shuffle(all_idx)\n",
    "\n",
    "test_idx  = sorted(all_idx[:TEST_N])\n",
    "val_idx   = sorted(all_idx[TEST_N:TEST_N + VAL_N])\n",
    "train_idx = sorted(all_idx[TEST_N + VAL_N:])\n",
    "\n",
    "print(f\"[SPLIT] Train={len(train_idx)}  Val={len(val_idx)}  Test={len(test_idx)}  (Total N={N})\")\n",
    "\n",
    "# 4) Subsets\n",
    "train_ds = Subset(ds, train_idx)\n",
    "val_ds   = Subset(ds, val_idx)\n",
    "test_ds  = Subset(ds, test_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7540bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Collators & loaders\n",
    "# -----------------------------------\n",
    "\n",
    "# TRAIN collator: stochastic masking + BOTH balancing strategies\n",
    "train_cfg = MaskingConfig(\n",
    "    mlm_prob=mlm_prob,\n",
    "    prob_joint=prob_joint,\n",
    "    prob_otu_only=prob_otu_only,\n",
    "    prob_tax_only=prob_tax_only,\n",
    "    max_len=max_len,\n",
    "    keep_prob=keep_prob,\n",
    "    random_prob=random_prob,\n",
    "    balance_mode=\"otu\",\n",
    ")\n",
    "\n",
    "train_collate = make_collator_balanced(\n",
    "    dataset=ds,\n",
    "    cfg=train_cfg,\n",
    ")\n",
    "\n",
    "# VAL / TEST collator:\n",
    "# Recommended: keep masking same, but DISABLE balancing (so evaluation is not distribution-shaped).\n",
    "val_cfg = MaskingConfig(\n",
    "    mlm_prob=mlm_prob,\n",
    "    prob_joint=prob_joint,\n",
    "    prob_otu_only=prob_otu_only,\n",
    "    prob_tax_only=prob_tax_only,\n",
    "    max_len=max_len,\n",
    "    keep_prob=keep_prob,\n",
    "    random_prob=random_prob,\n",
    "    balance_mode=\"none\",    # important: no endpoint re-selection for eval\n",
    ")\n",
    "\n",
    "val_collate = make_collator_balanced(\n",
    "    dataset=ds,\n",
    "    cfg=val_cfg,\n",
    ")\n",
    "\n",
    "test_collate = make_collator_balanced(\n",
    "    dataset=ds,\n",
    "    cfg=val_cfg,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=train_collate,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=val_batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=val_collate,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=val_batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=test_collate,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "# -----------------------------------\n",
    "# Model sizes & PAD ids (single place)\n",
    "# -----------------------------------\n",
    "\n",
    "# OTUs: unchanged\n",
    "n_otus = ds.O + 2            # + pad, mask\n",
    "pad_otu_id = ds.O\n",
    "\n",
    "# TAXA (UNK-aware):\n",
    "# - T_real = original number of taxa (from taxonomy_vocab.json)\n",
    "# - T_base = len(tax_vocab_unk) = T_real + 7 (real + UNKs)\n",
    "# - n_taxa = T_base + 2 (PAD, MASK at the end)\n",
    "n_taxa = T_base + 2\n",
    "pad_tax_id  = T_base         # PAD token index\n",
    "mask_tax_id = T_base + 1     # MASK token index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66e8b6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BATCH INSPECTION ===\n",
      "sample_id <class 'list'>\n",
      "input_otus torch.Size([256, 500]) torch.int64\n",
      "input_taxa torch.Size([256, 500]) torch.int64\n",
      "labels_otu torch.Size([256, 500]) torch.int64\n",
      "labels_taxa torch.Size([256, 500]) torch.int64\n",
      "attention_mask torch.Size([256, 500]) torch.int64\n",
      "lengths torch.Size([256]) torch.int64\n",
      "special_ids <class 'dict'>\n",
      "\n",
      "--- First sample in batch ---\n",
      "input_otus[:20]: tensor([  700,   408,   255, 62201,   563,  8336,   562,  3594, 14507,   303,\n",
      "         1211, 19666, 62201,  1176,   170,   559, 62201,  5342,  1104,    65])\n",
      "input_taxa[:20]: tensor([2041, 2343, 2160,  393, 3158, 3551, 2343, 1197, 3012, 2343, 2343,  328,\n",
      "        2718, 3012, 3116,  371,  679,  508, 3037, 2400])\n",
      "labels_otu[:20]: tensor([ -100,  -100,  -100,  5380,  -100,  9488,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  3924,  -100,  -100,  -100, 17936,  -100,  -100,  -100])\n",
      "labels_taxa[:20]: tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100])\n",
      "attention_mask[:20]: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Take ONE batch from the train loader\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"\\n=== BATCH INSPECTION ===\")\n",
    "for k, v in batch.items():\n",
    "    if torch.is_tensor(v):\n",
    "        print(k, v.shape, v.dtype)\n",
    "    else:\n",
    "        print(k, type(v))\n",
    "\n",
    "# Look at first sample in the batch\n",
    "b = 0\n",
    "print(\"\\n--- First sample in batch ---\")\n",
    "print(\"input_otus[:20]:\", batch[\"input_otus\"][b, :20])\n",
    "print(\"input_taxa[:20]:\", batch[\"input_taxa\"][b, :20])\n",
    "print(\"labels_otu[:20]:\", batch[\"labels_otu\"][b, :20])\n",
    "print(\"labels_taxa[:20]:\", batch[\"labels_taxa\"][b, :20])\n",
    "print(\"attention_mask[:20]:\", batch[\"attention_mask\"][b, :20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8a64966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CHECK FOR -1 ===\n",
      "[WARNING] input_taxa contains -1 at [[8, 288], [12, 0], [13, 290], [18, 7], [24, 0], [24, 10], [30, 5], [30, 13], [30, 16], [30, 76]]\n",
      "[WARNING] labels_taxa contains -1 at [[12, 6], [41, 6], [43, 49], [43, 92], [43, 145], [43, 182], [44, 14], [53, 72], [53, 141], [56, 20]]\n"
     ]
    }
   ],
   "source": [
    "def check_neg_ones(name, tensor):\n",
    "    if not torch.is_tensor(tensor):\n",
    "        return\n",
    "    mask = (tensor == -1)\n",
    "    if mask.any():\n",
    "        idx = torch.nonzero(mask, as_tuple=False)\n",
    "        print(f\"[WARNING] {name} contains -1 at {idx[:10].tolist()}\")\n",
    "    else:\n",
    "        print(f\"[OK] {name} contains no -1\")\n",
    "\n",
    "print(\"\\n=== CHECK FOR -1 ===\")\n",
    "check_neg_ones(\"input_taxa\", batch[\"input_taxa\"])\n",
    "check_neg_ones(\"labels_taxa\", batch[\"labels_taxa\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908d2303",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4a5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_ontology_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

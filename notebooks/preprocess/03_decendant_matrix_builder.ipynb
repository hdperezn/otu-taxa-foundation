{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0730ceb8",
   "metadata": {},
   "source": [
    "# Descendant matrix builder (taxonomy ancestor/descendant closure)\n",
    "\n",
    "we build:\n",
    "   - parent.npy / depth.npy / root_id.npy  (optional but useful)\n",
    "   - descendant_matrix.npy                (M: ancestor/descendant closure)\n",
    "\n",
    " Matrix definition (same as your old ETE3 version):\n",
    "   M[i, j] = 1  iff  node j is node i OR a descendant of node i\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5661bdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_DIR = /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_silva_sintax/dataset_full_top999\n",
      "TREE_DIR    = /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_silva_sintax/dataset_full_top999/tree_artifacts\n",
      "N taxonomy nodes = 6928\n",
      "Top-level clades: ['k:Bacteria', 'k:Archaea']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "# --------------------------\n",
    "# Paths\n",
    "# --------------------------\n",
    "DATASET_DIR = Path(\n",
    "    \"/home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training\"\n",
    "    \"/level_97/silva-138.2/incomplete_silva_sintax/dataset_full_top999\"\n",
    ")\n",
    "\n",
    "VOCAB_PATH  = DATASET_DIR / \"taxonomy_vocab.json\"\n",
    "NESTED_PATH = DATASET_DIR / \"taxonomy_nested.json\"\n",
    "\n",
    "assert VOCAB_PATH.exists(), f\"Missing: {VOCAB_PATH}\"\n",
    "assert NESTED_PATH.exists(), f\"Missing: {NESTED_PATH}\"\n",
    "\n",
    "# Save derived matrices into a subfolder (recommended)\n",
    "TREE_DIR = DATASET_DIR / \"tree_artifacts\"\n",
    "TREE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATASET_DIR =\", DATASET_DIR)\n",
    "print(\"TREE_DIR    =\", TREE_DIR)\n",
    "\n",
    "# --------------------------\n",
    "# Load vocab + nested tree\n",
    "# --------------------------\n",
    "names = json.load(open(VOCAB_PATH, \"r\"))\n",
    "name2idx = {nm: i for i, nm in enumerate(names)}\n",
    "N = len(names)\n",
    "\n",
    "taxonomy_nested = json.load(open(NESTED_PATH, \"r\"))\n",
    "\n",
    "print(\"N taxonomy nodes =\", N)\n",
    "print(\"Top-level clades:\", list(taxonomy_nested.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a40b0291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of roots (top-level clades) = 2\n",
      "First 10 roots: ['k:Archaea', 'k:Bacteria']\n",
      "Depth computed for all nodes.\n",
      "Depth min/max: 0 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building descendant matrix (dense): 100%|██████████| 6928/6928 [00:00<00:00, 133075.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descendant matrix shape: (6928, 6928) dtype: uint8\n",
      "Diagonal sum (should be N): 6928\n",
      "Nonzero entries: 41663\n",
      "Saved: /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_silva_sintax/dataset_full_top999/tree_artifacts/descendant_matrix.npy\n",
      "Saved: /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_silva_sintax/dataset_full_top999/tree_artifacts/README_descendants.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 1) Build parent pointers and children adjacency from nested dict\n",
    "# ============================================================\n",
    "\n",
    "def build_parent_children_from_nested(taxonomy_nested, name2idx):\n",
    "    \"\"\"\n",
    "    Build parent pointers and children adjacency using taxonomy_nested.json.\n",
    "\n",
    "    parent[i] = parent index of node i, or -1 if i is a top-level clade\n",
    "    children[i] = list of child indices\n",
    "    \"\"\"\n",
    "    N = len(name2idx)\n",
    "    parent = np.full(N, -1, dtype=np.int32)\n",
    "    children = [[] for _ in range(N)]\n",
    "\n",
    "    def dfs(subtree: dict, parent_name=None):\n",
    "        for node_name, child_dict in subtree.items():\n",
    "            if node_name not in name2idx:\n",
    "                # Should not happen if nested tree was built from vocab;\n",
    "                # keep simple and skip.\n",
    "                continue\n",
    "\n",
    "            i = name2idx[node_name]\n",
    "\n",
    "            if parent_name is not None and parent_name in name2idx:\n",
    "                p = name2idx[parent_name]\n",
    "                parent[i] = p\n",
    "                children[p].append(i)\n",
    "\n",
    "            if isinstance(child_dict, dict) and child_dict:\n",
    "                dfs(child_dict, parent_name=node_name)\n",
    "\n",
    "    # forest: multiple top-level clades allowed\n",
    "    dfs(taxonomy_nested, parent_name=None)\n",
    "    return parent, children\n",
    "\n",
    "\n",
    "parent, children = build_parent_children_from_nested(taxonomy_nested, name2idx)\n",
    "roots = np.where(parent == -1)[0].tolist()\n",
    "\n",
    "print(\"Number of roots (top-level clades) =\", len(roots))\n",
    "print(\"First 10 roots:\", [names[i] for i in roots[:10]])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Compute depth (optional but convenient for debugging)\n",
    "# ============================================================\n",
    "\n",
    "def compute_depth(children, roots):\n",
    "    depth = np.full(len(children), -1, dtype=np.int32)\n",
    "    q = deque()\n",
    "    for r in roots:\n",
    "        depth[r] = 0\n",
    "        q.append(r)\n",
    "    while q:\n",
    "        u = q.popleft()\n",
    "        for v in children[u]:\n",
    "            depth[v] = depth[u] + 1\n",
    "            q.append(v)\n",
    "    return depth\n",
    "\n",
    "\n",
    "depth = compute_depth(children, roots)\n",
    "\n",
    "if (depth < 0).any():\n",
    "    bad = np.where(depth < 0)[0]\n",
    "    print(\"WARNING: some nodes unreachable (unexpected). Count:\", len(bad))\n",
    "    print(\"Examples:\", [names[i] for i in bad[:10]])\n",
    "else:\n",
    "    print(\"Depth computed for all nodes.\")\n",
    "    print(\"Depth min/max:\", int(depth.min()), int(depth.max()))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Build descendant-closure matrix M\n",
    "#    M[i, j] = 1 iff j is i or a descendant of i (same as old ETE3 code)\n",
    "# ============================================================\n",
    "\n",
    "def euler_tour_times(children, roots):\n",
    "    \"\"\"\n",
    "    Compute entry/exit times (tin/tout) for each node.\n",
    "    In a rooted tree/forest:\n",
    "      i is ancestor of j  <=>  tin[i] <= tin[j] and tout[j] <= tout[i]\n",
    "    \"\"\"\n",
    "    N = len(children)\n",
    "    tin = np.full(N, -1, dtype=np.int32)\n",
    "    tout = np.full(N, -1, dtype=np.int32)\n",
    "    t = 0\n",
    "\n",
    "    def dfs(u):\n",
    "        nonlocal t\n",
    "        tin[u] = t\n",
    "        t += 1\n",
    "        for v in children[u]:\n",
    "            dfs(v)\n",
    "        tout[u] = t\n",
    "        t += 1\n",
    "\n",
    "    for r in roots:\n",
    "        dfs(r)\n",
    "\n",
    "    return tin, tout\n",
    "\n",
    "\n",
    "tin, tout = euler_tour_times(children, roots)\n",
    "\n",
    "def build_descendant_matrix_dense(tin, tout, dtype=np.uint8):\n",
    "    \"\"\"\n",
    "    Dense descendant-closure matrix:\n",
    "      M[i, j] = 1 iff i is ancestor of j (including i==j)\n",
    "    \"\"\"\n",
    "    N = len(tin)\n",
    "    M = np.zeros((N, N), dtype=dtype)\n",
    "    for i in tqdm(range(N), desc=\"Building descendant matrix (dense)\"):\n",
    "        # vectorized ancestor test across all j\n",
    "        M[i, :] = ((tin[i] <= tin) & (tout <= tout[i])).astype(dtype)\n",
    "    return M\n",
    "\n",
    "\n",
    "M = build_descendant_matrix_dense(tin, tout, dtype=np.uint8)\n",
    "\n",
    "print(\"Descendant matrix shape:\", M.shape, \"dtype:\", M.dtype)\n",
    "print(\"Diagonal sum (should be N):\", int(np.trace(M)))\n",
    "print(\"Nonzero entries:\", int(M.sum()))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Save artifacts\n",
    "# ============================================================\n",
    "\n",
    "np.save(TREE_DIR / \"parent.npy\", parent)\n",
    "np.save(TREE_DIR / \"depth.npy\", depth)\n",
    "np.save(TREE_DIR / \"tin.npy\", tin)\n",
    "np.save(TREE_DIR / \"tout.npy\", tout)\n",
    "\n",
    "desc_path = TREE_DIR / \"descendant_matrix.npy\"\n",
    "np.save(desc_path, M)\n",
    "print(\"Saved:\", desc_path)\n",
    "\n",
    "# Minimal README for this folder (optional but recommended)\n",
    "readme_path = TREE_DIR / \"README_descendants.md\"\n",
    "with open(readme_path, \"w\") as f:\n",
    "    f.write(\n",
    "        \"# Descendant matrix (ancestor/descendant closure)\\n\\n\"\n",
    "        \"Files in this folder are derived from taxonomy_nested.json and taxonomy_vocab.json.\\n\\n\"\n",
    "        \"## descendant_matrix.npy\\n\"\n",
    "        \"Let `names = taxonomy_vocab.json` (length N).\\n\"\n",
    "        \"The matrix `M` has shape [N, N] and is defined as:\\n\"\n",
    "        \"`M[i, j] = 1` iff node `names[j]` is equal to or a descendant of node `names[i]`.\\n\\n\"\n",
    "        \"This matches the old ETE3 implementation, but uses an Euler tour (tin/tout) for speed.\\n\"\n",
    "    )\n",
    "print(\"Saved:\", readme_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd4ed9",
   "metadata": {},
   "source": [
    "# testing decendant matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53bafbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DESCENDANTS of 'g:Roseburia' (including itself):\n",
      "Total: 3\n",
      "  - g:Roseburia\n",
      "  - s:Lachnospiraceae_bacterium_feline_oral_taxon_021\n",
      "  - s:Roseburia_sp._499\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def show_descendants(label, names, M_tensor, max_print=50):\n",
    "    \"\"\"\n",
    "    Print all descendants of taxonomy label using the descendant matrix M.\n",
    "    - label: string (e.g., 'f:Lachnospiraceae')\n",
    "    - names: vocab list (taxonomy_vocab.json)\n",
    "    - M_tensor: descendant closure matrix [N, N] (numpy array or torch tensor)\n",
    "    \"\"\"\n",
    "    # handle numpy or torch\n",
    "    if hasattr(M_tensor, \"cpu\"):  # torch tensor\n",
    "        row = M_tensor[names.index(label)].cpu().numpy()\n",
    "    else:  # numpy\n",
    "        row = M_tensor[names.index(label)]\n",
    "\n",
    "    descendant_indices = np.where(row == 1)[0]\n",
    "    descendant_names = [names[j] for j in descendant_indices]\n",
    "\n",
    "    print(f\"\\nDESCENDANTS of '{label}' (including itself):\")\n",
    "    print(f\"Total: {len(descendant_names)}\")\n",
    "\n",
    "    for nm in descendant_names[:max_print]:\n",
    "        print(\"  -\", nm)\n",
    "\n",
    "    if len(descendant_names) > max_print:\n",
    "        print(f\"... (showing first {max_print} of {len(descendant_names)})\")\n",
    "\n",
    "\n",
    "# Example inspections (edit freely)\n",
    "#show_descendants(\"f:Lachnospiraceae\", names, M, max_print=50)\n",
    "show_descendants(\"g:Roseburia\", names, M, max_print=50)\n",
    "#show_descendants(\"f:Veillonellaceae\", names, M, max_print=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac9188",
   "metadata": {},
   "source": [
    "# Extending the taxonomy tree with per-rank UNK nodes\n",
    "\n",
    "This section augments the base taxonomy tree with one UNK node per rank \n",
    "\n",
    "(k:UNK, p:UNK, ..., s:UNK) in order to support per-rank taxonomy prediction\n",
    "\n",
    "and hierarchical regularization in the OTU+Taxa foundation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aba4a867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded base taxonomy: T_old=6928\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Paths (same tree_artifacts directory)\n",
    "# --------------------------\n",
    "TREE_DIR = DATASET_DIR / \"tree_artifacts\"\n",
    "\n",
    "BASE_VOCAB_PATH = DATASET_DIR / \"taxonomy_vocab.json\"\n",
    "BASE_M_PATH     = TREE_DIR / \"descendant_matrix.npy\"\n",
    "\n",
    "NEW_VOCAB_PATH  = TREE_DIR / \"taxonomy_vocab_with_unk.json\"\n",
    "NEW_M_PATH      = TREE_DIR / \"descendant_matrix_with_unk.npy\"\n",
    "RANK_IDX_PATH   = TREE_DIR / \"rank_idx.npy\"\n",
    "\n",
    "# --------------------------\n",
    "# Load base vocab and matrix\n",
    "# --------------------------\n",
    "with open(BASE_VOCAB_PATH, \"r\") as f:\n",
    "    vocab_old = json.load(f)\n",
    "\n",
    "M_old = np.load(BASE_M_PATH)\n",
    "T_old = len(vocab_old)\n",
    "\n",
    "assert M_old.shape == (T_old, T_old), \"Mismatch between vocab and descendant matrix\"\n",
    "\n",
    "print(f\"Loaded base taxonomy: T_old={T_old}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d407353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended vocab saved: T_new=6935\n",
      "Saved UNK-extended hierarchy artifacts:\n",
      " - /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_silva_sintax/dataset_full_top999/tree_artifacts/taxonomy_vocab_with_unk.json\n",
      " - /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_silva_sintax/dataset_full_top999/tree_artifacts/descendant_matrix_with_unk.npy\n",
      " - /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_silva_sintax/dataset_full_top999/tree_artifacts/rank_idx.npy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------------\n",
    "# Rank mapping\n",
    "# --------------------------\n",
    "RANK_CHAR_TO_IDX = {\"k\":0, \"p\":1, \"c\":2, \"o\":3, \"f\":4, \"g\":5, \"s\":6}\n",
    "R = 7\n",
    "\n",
    "rank_idx_old = np.empty(T_old, dtype=np.int64)\n",
    "for i, name in enumerate(vocab_old):\n",
    "    c = name[0].lower()\n",
    "    if c not in RANK_CHAR_TO_IDX:\n",
    "        raise ValueError(f\"Cannot infer rank from token: {name}\")\n",
    "    rank_idx_old[i] = RANK_CHAR_TO_IDX[c]\n",
    "\n",
    "# --------------------------\n",
    "# Extend vocabulary\n",
    "# --------------------------\n",
    "unk_tokens = [f\"{r}:UNK\" for r in [\"k\",\"p\",\"c\",\"o\",\"f\",\"g\",\"s\"]]\n",
    "vocab_new = vocab_old + unk_tokens\n",
    "T_new = len(vocab_new)\n",
    "\n",
    "with open(NEW_VOCAB_PATH, \"w\") as f:\n",
    "    json.dump(vocab_new, f, indent=2)\n",
    "\n",
    "print(f\"Extended vocab saved: T_new={T_new}\")\n",
    "\n",
    "# --------------------------\n",
    "# Build extended descendant matrix\n",
    "# --------------------------\n",
    "M_new = np.zeros((T_new, T_new), dtype=M_old.dtype)\n",
    "M_new[:T_old, :T_old] = M_old\n",
    "\n",
    "# Build rank_idx for extended vocab\n",
    "rank_idx_new = np.empty(T_new, dtype=np.int64)\n",
    "rank_idx_new[:T_old] = rank_idx_old\n",
    "for r in range(R):\n",
    "    rank_idx_new[T_old + r] = r\n",
    "\n",
    "# 1) Link each UNK_r as child of ALL real nodes at rank r-1\n",
    "for r in range(1, R):\n",
    "    unk_id = T_old + r\n",
    "    parents = np.where(rank_idx_old == (r - 1))[0]\n",
    "    if parents.size == 0:\n",
    "        print(f\"[WARN] No parents found at rank {r-1} for UNK_{r}\")\n",
    "        continue\n",
    "    M_new[parents, unk_id] = 1\n",
    "\n",
    "# 2) Chain UNKs: UNK_k → UNK_p → ... → UNK_s\n",
    "for r in range(R - 1):\n",
    "    parent_unk = T_old + r\n",
    "    child_unk  = T_old + (r + 1)\n",
    "    M_new[parent_unk, child_unk] = 1\n",
    "\n",
    "# 3) Closure: self-descendants\n",
    "np.fill_diagonal(M_new, 1)\n",
    "\n",
    "# --------------------------\n",
    "# Save artifacts\n",
    "# --------------------------\n",
    "np.save(NEW_M_PATH, M_new)\n",
    "np.save(RANK_IDX_PATH, rank_idx_new)\n",
    "\n",
    "print(\"Saved UNK-extended hierarchy artifacts:\")\n",
    "print(\" -\", NEW_VOCAB_PATH)\n",
    "print(\" -\", NEW_M_PATH)\n",
    "print(\" -\", RANK_IDX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f930d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK tokens:\n",
      "6928: k:UNK\n",
      "6929: p:UNK\n",
      "6930: c:UNK\n",
      "6931: o:UNK\n",
      "6932: f:UNK\n",
      "6933: g:UNK\n",
      "6934: s:UNK\n",
      "\n",
      "7×7 descendant matrix for UNK tokens (rows=ancestors, cols=descendants):\n",
      "[[1 1 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [0 0 1 1 0 0 0]\n",
      " [0 0 0 1 1 0 0]\n",
      " [0 0 0 0 1 1 0]\n",
      " [0 0 0 0 0 1 1]\n",
      " [0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Load the extended vocab and matrix\n",
    "vocab_new = json.load(open(NEW_VOCAB_PATH))\n",
    "M_new = np.load(NEW_M_PATH)\n",
    "\n",
    "T_new = len(vocab_new)\n",
    "T_old = T_new - 7   # since last 7 tokens are the UNKs\n",
    "\n",
    "# Extract the 7×7 UNK submatrix\n",
    "M_unk = M_new[T_old:T_new, T_old:T_new]\n",
    "\n",
    "print(\"UNK tokens:\")\n",
    "for i, name in enumerate(vocab_new[T_old:T_new]):\n",
    "    print(f\"{T_old+i}: {name}\")\n",
    "\n",
    "print(\"\\n7x7 descendant matrix for UNK tokens (rows=ancestors, cols=descendants):\")\n",
    "print(M_unk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb0c091f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_old = 6928, T_new = 6935\n",
      "Index of 'p:UNK' = 6929\n",
      "\n",
      "Parents of 'p:UNK' (rows where M_new[parent, 'p:UNK'] == 1):\n",
      "    3494  k:Archaea\n",
      "    3495  k:Bacteria\n",
      "    6928  k:UNK\n",
      "    6929  p:UNK\n",
      "\n",
      "Check specific expected parents:\n",
      "  k:Archaea   idx=  3494  -> p:UNK edge: True\n",
      "  k:Bacteria  idx=  3495  -> p:UNK edge: True\n",
      "  k:UNK       idx=  6928  -> p:UNK edge: True\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Load vocab + matrix\n",
    "# ----------------------------\n",
    "vocab_new = json.load(open(NEW_VOCAB_PATH))\n",
    "M_new = np.load(NEW_M_PATH)\n",
    "\n",
    "T_new = len(vocab_new)\n",
    "T_old = T_new - 7   # assuming last 7 tokens are the UNKs\n",
    "\n",
    "print(f\"T_old = {T_old}, T_new = {T_new}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Find index of p:UNK\n",
    "# ----------------------------\n",
    "try:\n",
    "    idx_p_unk = vocab_new.index(\"p:UNK\")\n",
    "except ValueError:\n",
    "    raise ValueError(\"'p:UNK' not found in vocab_new. Check naming.\")\n",
    "\n",
    "print(f\"Index of 'p:UNK' = {idx_p_unk}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Find all parents of p:UNK\n",
    "# (rows where M_new[parent, idx_p_unk] == 1)\n",
    "# ----------------------------\n",
    "parents_idx = np.where(M_new[:, idx_p_unk] == 1)[0]\n",
    "\n",
    "print(\"\\nParents of 'p:UNK' (rows where M_new[parent, 'p:UNK'] == 1):\")\n",
    "for p in parents_idx:\n",
    "    print(f\"  {p:6d}  {vocab_new[p]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# OPTIONAL: check specifically Archaea, Bacteria, k:UNK\n",
    "# ----------------------------\n",
    "names_to_check = [\"k:Archaea\", \"k:Bacteria\", \"k:UNK\"]\n",
    "print(\"\\nCheck specific expected parents:\")\n",
    "for name in names_to_check:\n",
    "    if name in vocab_new:\n",
    "        idx = vocab_new.index(name)\n",
    "        connected = bool(M_new[idx, idx_p_unk] == 1)\n",
    "        print(f\"  {name:10s}  idx={idx:6d}  -> p:UNK edge: {connected}\")\n",
    "    else:\n",
    "        print(f\"  {name:10s}  NOT FOUND in vocab_new\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_ontology_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

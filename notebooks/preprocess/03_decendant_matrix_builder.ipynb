{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0730ceb8",
   "metadata": {},
   "source": [
    "# Descendant matrix builder (taxonomy ancestor/descendant closure)\n",
    "\n",
    "we build:\n",
    "   - parent.npy / depth.npy / root_id.npy  (optional but useful)\n",
    "   - descendant_matrix.npy                (M: ancestor/descendant closure)\n",
    "\n",
    " Matrix definition (same as your old ETE3 version):\n",
    "   M[i, j] = 1  iff  node j is node i OR a descendant of node i\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5661bdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_DIR = /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_silva_sintax/dataset_full_top999\n",
      "TREE_DIR    = /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_silva_sintax/dataset_full_top999/tree_artifacts\n",
      "N taxonomy nodes = 6929\n",
      "Top-level clades: ['k:Bacteria', 'k:Archaea']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "# --------------------------\n",
    "# Paths\n",
    "# --------------------------\n",
    "DATASET_DIR = Path(\n",
    "    \"/home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training\"\n",
    "    \"/level_97/silva-138.2/incomplete_silva_sintax/dataset_full_top999\"\n",
    ")\n",
    "\n",
    "VOCAB_PATH  = DATASET_DIR / \"taxonomy_vocab.json\"\n",
    "NESTED_PATH = DATASET_DIR / \"taxonomy_nested.json\"\n",
    "\n",
    "assert VOCAB_PATH.exists(), f\"Missing: {VOCAB_PATH}\"\n",
    "assert NESTED_PATH.exists(), f\"Missing: {NESTED_PATH}\"\n",
    "\n",
    "# Save derived matrices into a subfolder (recommended)\n",
    "TREE_DIR = DATASET_DIR / \"tree_artifacts\"\n",
    "TREE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATASET_DIR =\", DATASET_DIR)\n",
    "print(\"TREE_DIR    =\", TREE_DIR)\n",
    "\n",
    "# --------------------------\n",
    "# Load vocab + nested tree\n",
    "# --------------------------\n",
    "names = json.load(open(VOCAB_PATH, \"r\"))\n",
    "name2idx = {nm: i for i, nm in enumerate(names)}\n",
    "N = len(names)\n",
    "\n",
    "taxonomy_nested = json.load(open(NESTED_PATH, \"r\"))\n",
    "\n",
    "print(\"N taxonomy nodes =\", N)\n",
    "print(\"Top-level clades:\", list(taxonomy_nested.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b0291",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'taxonomy_nested' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m     dfs(taxonomy_nested, parent_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parent, children\n\u001b[0;32m---> 38\u001b[0m parent, children \u001b[38;5;241m=\u001b[39m build_parent_children_from_nested(\u001b[43mtaxonomy_nested\u001b[49m, name2idx)\n\u001b[1;32m     39\u001b[0m roots \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mwhere(parent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mif\u001b[39;00m names[i] \u001b[38;5;129;01min\u001b[39;00m taxonomy_nested]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of roots (top-level clades) =\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(roots))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'taxonomy_nested' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 1) Build parent pointers and children adjacency from nested dict\n",
    "# ============================================================\n",
    "\n",
    "def build_parent_children_from_nested(taxonomy_nested, name2idx):\n",
    "    \"\"\"\n",
    "    Build parent pointers and children adjacency using taxonomy_nested.json.\n",
    "\n",
    "    parent[i] = parent index of node i, or -1 if i is a top-level clade\n",
    "    children[i] = list of child indices\n",
    "    \"\"\"\n",
    "    N = len(name2idx)\n",
    "    parent = np.full(N, -1, dtype=np.int32)\n",
    "    children = [[] for _ in range(N)]\n",
    "\n",
    "    def dfs(subtree: dict, parent_name=None):\n",
    "        for node_name, child_dict in subtree.items():\n",
    "            if node_name not in name2idx:\n",
    "                # Should not happen if nested tree was built from vocab;\n",
    "                # keep simple and skip.\n",
    "                continue\n",
    "\n",
    "            i = name2idx[node_name]\n",
    "\n",
    "            if parent_name is not None and parent_name in name2idx:\n",
    "                p = name2idx[parent_name]\n",
    "                parent[i] = p\n",
    "                children[p].append(i)\n",
    "\n",
    "            if isinstance(child_dict, dict) and child_dict:\n",
    "                dfs(child_dict, parent_name=node_name)\n",
    "\n",
    "    # forest: multiple top-level clades allowed\n",
    "    dfs(taxonomy_nested, parent_name=None)\n",
    "    return parent, children\n",
    "\n",
    "\n",
    "parent, children = build_parent_children_from_nested(taxonomy_nested, name2idx)\n",
    "roots = [i for i in np.where(parent == -1)[0].tolist() if names[i] in taxonomy_nested]\n",
    "\n",
    "print(\"Number of roots (top-level clades) =\", len(roots))\n",
    "print(\"First 10 roots:\", [names[i] for i in roots[:10]])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Compute depth (optional but convenient for debugging)\n",
    "# ============================================================\n",
    "\n",
    "def compute_depth(children, roots):\n",
    "    \"\"\"\n",
    "    BFS depth from each root.\n",
    "    Any unreachable nodes (e.g., appended k:UNK) are assigned depth=0\n",
    "    and treated as singleton components.\n",
    "    \"\"\"\n",
    "    N = len(children)\n",
    "    depth = np.full(N, -1, dtype=np.int32)\n",
    "\n",
    "    q = deque()\n",
    "    for r in roots:\n",
    "        depth[r] = 0\n",
    "        q.append(r)\n",
    "\n",
    "    while q:\n",
    "        u = q.popleft()\n",
    "        for v in children[u]:\n",
    "            depth[v] = depth[u] + 1\n",
    "            q.append(v)\n",
    "\n",
    "    # Unreachable nodes: set depth=0 (singleton)\n",
    "    bad = np.where(depth < 0)[0]\n",
    "    if bad.size > 0:\n",
    "        depth[bad] = 0\n",
    "\n",
    "    return depth\n",
    "\n",
    "\n",
    "\n",
    "depth = compute_depth(children, roots)\n",
    "\n",
    "if (depth < 0).any():\n",
    "    bad = np.where(depth < 0)[0]\n",
    "    print(\"WARNING: some nodes unreachable (unexpected). Count:\", len(bad))\n",
    "    print(\"Examples:\", [names[i] for i in bad[:10]])\n",
    "else:\n",
    "    print(\"Depth computed for all nodes.\")\n",
    "    print(\"Depth min/max:\", int(depth.min()), int(depth.max()))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Build descendant-closure matrix M\n",
    "#    M[i, j] = 1 iff j is i or a descendant of i (same as old ETE3 code)\n",
    "# ============================================================\n",
    "\n",
    "def euler_tour_times(children, roots):\n",
    "    \"\"\"\n",
    "    Compute entry/exit times (tin/tout) for each node.\n",
    "\n",
    "    For nodes reachable from the forest roots, we do a DFS Euler tour.\n",
    "    For unreachable nodes (e.g., appended k:UNK), we assign them unique\n",
    "    singleton intervals so that:\n",
    "      - they are ancestor/descendant only of themselves\n",
    "      - they do not become ancestor/descendant of real nodes by accident\n",
    "    \"\"\"\n",
    "    N = len(children)\n",
    "    tin = np.full(N, -1, dtype=np.int32)\n",
    "    tout = np.full(N, -1, dtype=np.int32)\n",
    "    t = 0\n",
    "\n",
    "    def dfs(u):\n",
    "        nonlocal t\n",
    "        tin[u] = t\n",
    "        t += 1\n",
    "        for v in children[u]:\n",
    "            dfs(v)\n",
    "        tout[u] = t\n",
    "        t += 1\n",
    "\n",
    "    # DFS on reachable forest\n",
    "    for r in roots:\n",
    "        if tin[r] == -1:\n",
    "            dfs(r)\n",
    "\n",
    "    # Assign singleton Euler intervals to unreachable nodes\n",
    "    bad = np.where(tin < 0)[0]\n",
    "    for u in bad:\n",
    "        tin[u] = t\n",
    "        t += 1\n",
    "        tout[u] = t\n",
    "        t += 1\n",
    "\n",
    "    return tin, tout\n",
    "\n",
    "\n",
    "tin, tout = euler_tour_times(children, roots)\n",
    "\n",
    "def build_descendant_matrix_dense(tin, tout, dtype=np.uint8):\n",
    "    \"\"\"\n",
    "    Dense descendant-closure matrix:\n",
    "      M[i, j] = 1 iff i is ancestor of j (including i==j)\n",
    "    \"\"\"\n",
    "    N = len(tin)\n",
    "    M = np.zeros((N, N), dtype=dtype)\n",
    "    for i in tqdm(range(N), desc=\"Building descendant matrix (dense)\"):\n",
    "        # vectorized ancestor test across all j\n",
    "        M[i, :] = ((tin[i] <= tin) & (tout <= tout[i])).astype(dtype)\n",
    "    return M\n",
    "\n",
    "\n",
    "M = build_descendant_matrix_dense(tin, tout, dtype=np.uint8)\n",
    "\n",
    "print(\"Descendant matrix shape:\", M.shape, \"dtype:\", M.dtype)\n",
    "print(\"Diagonal sum (should be N):\", int(np.trace(M)))\n",
    "print(\"Nonzero entries:\", int(M.sum()))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Save artifacts\n",
    "# ============================================================\n",
    "\n",
    "np.save(TREE_DIR / \"parent.npy\", parent)\n",
    "np.save(TREE_DIR / \"depth.npy\", depth)\n",
    "np.save(TREE_DIR / \"tin.npy\", tin)\n",
    "np.save(TREE_DIR / \"tout.npy\", tout)\n",
    "\n",
    "desc_path = TREE_DIR / \"descendant_matrix.npy\"\n",
    "np.save(desc_path, M)\n",
    "print(\"Saved:\", desc_path)\n",
    "\n",
    "# Minimal README for this folder (optional but recommended)\n",
    "readme_path = TREE_DIR / \"README_descendants.md\"\n",
    "with open(readme_path, \"w\") as f:\n",
    "    f.write(\n",
    "        \"# Descendant matrix (ancestor/descendant closure)\\n\\n\"\n",
    "        \"Files in this folder are derived from taxonomy_nested.json and taxonomy_vocab.json.\\n\\n\"\n",
    "        \"## descendant_matrix.npy\\n\"\n",
    "        \"Let `names = taxonomy_vocab.json` (length N).\\n\"\n",
    "        \"The matrix `M` has shape [N, N] and is defined as:\\n\"\n",
    "        \"`M[i, j] = 1` iff node `names[j]` is equal to or a descendant of node `names[i]`.\\n\\n\"\n",
    "        \"This matches the old ETE3 implementation, but uses an Euler tour (tin/tout) for speed.\\n\"\n",
    "    )\n",
    "print(\"Saved:\", readme_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5493cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parent, children = build_parent_children_from_nested(taxonomy_nested, name2idx)\n",
    "roots = [i for i in np.where(parent == -1)[0].tolist() if names[i] in taxonomy_nested]\n",
    "\n",
    "print(\"Number of roots (top-level clades) =\", len(roots))\n",
    "print(\"First 10 roots:\", [names[i] for i in roots[:10]])\n",
    "\n",
    "\n",
    "M = build_descendant_matrix_dense(tin, tout, dtype=np.uint8)\n",
    "\n",
    "print(\"Descendant matrix shape:\", M.shape, \"dtype:\", M.dtype)\n",
    "print(\"Diagonal sum (should be N):\", int(np.trace(M)))\n",
    "print(\"Nonzero entries:\", int(M.sum()))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Save artifacts\n",
    "# ============================================================\n",
    "\n",
    "np.save(TREE_DIR / \"parent.npy\", parent)\n",
    "np.save(TREE_DIR / \"depth.npy\", depth)\n",
    "np.save(TREE_DIR / \"tin.npy\", tin)\n",
    "np.save(TREE_DIR / \"tout.npy\", tout)\n",
    "\n",
    "desc_path = TREE_DIR / \"descendant_matrix.npy\"\n",
    "np.save(desc_path, M)\n",
    "print(\"Saved:\", desc_path)\n",
    "\n",
    "# Minimal README for this folder (optional but recommended)\n",
    "readme_path = TREE_DIR / \"README_descendants.md\"\n",
    "with open(readme_path, \"w\") as f:\n",
    "    f.write(\n",
    "        \"# Descendant matrix (ancestor/descendant closure)\\n\\n\"\n",
    "        \"Files in this folder are derived from taxonomy_nested.json and taxonomy_vocab.json.\\n\\n\"\n",
    "        \"## descendant_matrix.npy\\n\"\n",
    "        \"Let `names = taxonomy_vocab.json` (length N).\\n\"\n",
    "        \"The matrix `M` has shape [N, N] and is defined as:\\n\"\n",
    "        \"`M[i, j] = 1` iff node `names[j]` is equal to or a descendant of node `names[i]`.\\n\\n\"\n",
    "        \"This matches the old ETE3 implementation, but uses an Euler tour (tin/tout) for speed.\\n\"\n",
    "    )\n",
    "print(\"Saved:\", readme_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd4ed9",
   "metadata": {},
   "source": [
    "# testing decendant matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53bafbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DESCENDANTS of 'g:Roseburia' (including itself):\n",
      "Total: 3\n",
      "  - g:Roseburia\n",
      "  - s:Lachnospiraceae_bacterium_feline_oral_taxon_021\n",
      "  - s:Roseburia_sp._499\n",
      "\n",
      "DESCENDANTS of 'k:UNK' (including itself):\n",
      "Total: 1\n",
      "  - k:UNK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def show_descendants(label, names, M_tensor, max_print=50):\n",
    "    \"\"\"\n",
    "    Print all descendants of taxonomy label using the descendant matrix M.\n",
    "    - label: string (e.g., 'f:Lachnospiraceae')\n",
    "    - names: vocab list (taxonomy_vocab.json)\n",
    "    - M_tensor: descendant closure matrix [N, N] (numpy array or torch tensor)\n",
    "    \"\"\"\n",
    "    # handle numpy or torch\n",
    "    if hasattr(M_tensor, \"cpu\"):  # torch tensor\n",
    "        row = M_tensor[names.index(label)].cpu().numpy()\n",
    "    else:  # numpy\n",
    "        row = M_tensor[names.index(label)]\n",
    "\n",
    "    descendant_indices = np.where(row == 1)[0]\n",
    "    descendant_names = [names[j] for j in descendant_indices]\n",
    "\n",
    "    print(f\"\\nDESCENDANTS of '{label}' (including itself):\")\n",
    "    print(f\"Total: {len(descendant_names)}\")\n",
    "\n",
    "    for nm in descendant_names[:max_print]:\n",
    "        print(\"  -\", nm)\n",
    "\n",
    "    if len(descendant_names) > max_print:\n",
    "        print(f\"... (showing first {max_print} of {len(descendant_names)})\")\n",
    "\n",
    "\n",
    "# Example inspections (edit freely)\n",
    "#show_descendants(\"f:Lachnospiraceae\", names, M, max_print=50)\n",
    "show_descendants(\"g:Roseburia\", names, M, max_print=50)\n",
    "show_descendants(\"k:UNK\", names, M, max_print=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac9188",
   "metadata": {},
   "source": [
    "# Extending the taxonomy tree with per-rank UNK nodes\n",
    "\n",
    "This section augments the base taxonomy tree with one UNK node per rank \n",
    "\n",
    "(k:UNK, p:UNK, ..., s:UNK) in order to support per-rank taxonomy prediction\n",
    "\n",
    "and hierarchical regularization in the OTU+Taxa foundation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aba4a867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded base taxonomy: T_old=6929\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Paths (same tree_artifacts directory)\n",
    "# --------------------------\n",
    "TREE_DIR = DATASET_DIR / \"tree_artifacts\"\n",
    "\n",
    "BASE_VOCAB_PATH = DATASET_DIR / \"taxonomy_vocab.json\"\n",
    "BASE_M_PATH     = TREE_DIR / \"descendant_matrix.npy\"\n",
    "\n",
    "NEW_VOCAB_PATH  = TREE_DIR / \"taxonomy_vocab_with_unk.json\"\n",
    "NEW_M_PATH      = TREE_DIR / \"descendant_matrix_with_unk.npy\"\n",
    "RANK_IDX_PATH   = TREE_DIR / \"rank_idx.npy\"\n",
    "\n",
    "# --------------------------\n",
    "# Load base vocab and matrix\n",
    "# --------------------------\n",
    "with open(BASE_VOCAB_PATH, \"r\") as f:\n",
    "    vocab_old = json.load(f)\n",
    "\n",
    "M_old = np.load(BASE_M_PATH)\n",
    "T_old = len(vocab_old)\n",
    "\n",
    "assert M_old.shape == (T_old, T_old), \"Mismatch between vocab and descendant matrix\"\n",
    "\n",
    "print(f\"Loaded base taxonomy: T_old={T_old}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d407353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended vocab saved: T_new=6935 (added 6 UNKs)\n",
      "UNKs added: ['p:UNK', 'c:UNK', 'o:UNK', 'f:UNK', 'g:UNK', 's:UNK']\n",
      "Saved UNK-extended hierarchy artifacts:\n",
      " - /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_silva_sintax/dataset_full_top999/tree_artifacts/taxonomy_vocab_with_unk.json\n",
      " - /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_silva_sintax/dataset_full_top999/tree_artifacts/descendant_matrix_with_unk.npy\n",
      " - /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_silva_sintax/dataset_full_top999/tree_artifacts/rank_idx.npy\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Rank mapping (for base vocab)\n",
    "# --------------------------\n",
    "RANK_CHAR_TO_IDX = {\"k\":0, \"p\":1, \"c\":2, \"o\":3, \"f\":4, \"g\":5, \"s\":6}\n",
    "R = 7\n",
    "\n",
    "rank_idx_old = np.empty(T_old, dtype=np.int64)\n",
    "for i, name in enumerate(vocab_old):\n",
    "    c = name[0].lower()\n",
    "    if c not in RANK_CHAR_TO_IDX:\n",
    "        raise ValueError(f\"Cannot infer rank from token: {name}\")\n",
    "    rank_idx_old[i] = RANK_CHAR_TO_IDX[c]\n",
    "\n",
    "# --------------------------\n",
    "# Extend vocabulary (only add missing UNKs)\n",
    "#   Base already has k:UNK, so we only append p..s UNK if missing.\n",
    "# --------------------------\n",
    "desired_unk = [f\"{r}:UNK\" for r in [\"k\",\"p\",\"c\",\"o\",\"f\",\"g\",\"s\"]]\n",
    "have = set(vocab_old)\n",
    "\n",
    "unk_tokens_to_add = [tok for tok in desired_unk if tok not in have]\n",
    "vocab_new = vocab_old + unk_tokens_to_add\n",
    "T_new = len(vocab_new)\n",
    "\n",
    "with open(NEW_VOCAB_PATH, \"w\") as f:\n",
    "    json.dump(vocab_new, f, indent=2)\n",
    "\n",
    "print(f\"Extended vocab saved: T_new={T_new} (added {len(unk_tokens_to_add)} UNKs)\")\n",
    "print(\"UNKs added:\", unk_tokens_to_add)\n",
    "\n",
    "# Build rank_idx for extended vocab\n",
    "rank_idx_new = np.empty(T_new, dtype=np.int64)\n",
    "rank_idx_new[:T_old] = rank_idx_old\n",
    "\n",
    "for j, tok in enumerate(unk_tokens_to_add, start=T_old):\n",
    "    c = tok[0].lower()\n",
    "    rank_idx_new[j] = RANK_CHAR_TO_IDX[c]\n",
    "\n",
    "# --------------------------\n",
    "# Build extended descendant matrix\n",
    "# --------------------------\n",
    "M_new = np.zeros((T_new, T_new), dtype=M_old.dtype)\n",
    "M_new[:T_old, :T_old] = M_old\n",
    "\n",
    "# Helper: get UNK id for each rank from vocab_new\n",
    "unk_id_by_rank = {}\n",
    "for tok in desired_unk:\n",
    "    # guaranteed to exist either in base or appended\n",
    "    idx = vocab_new.index(tok)\n",
    "    unk_id_by_rank[tok[0].lower()] = idx\n",
    "\n",
    "# 1) Link each UNK_r (r>=1) as child of ALL REAL nodes at rank r-1\n",
    "#    NOTE: \"real nodes\" here means base vocabulary nodes. If base already\n",
    "#    contains k:UNK, it is NOT a \"real\" parent class; we typically exclude it.\n",
    "for r_char, r in [(\"p\",1), (\"c\",2), (\"o\",3), (\"f\",4), (\"g\",5), (\"s\",6)]:\n",
    "    unk_id = unk_id_by_rank[r_char]      # global id in vocab_new\n",
    "    parent_rank = r - 1\n",
    "\n",
    "    # parents among BASE nodes only (0..T_old-1)\n",
    "    parents = np.where(rank_idx_old == parent_rank)[0]\n",
    "\n",
    "    # optional: exclude k:UNK from being a parent of p:UNK\n",
    "    # (base may include k:UNK; we don't want it in \"parents\")\n",
    "    if parent_rank == 0 and \"k:UNK\" in vocab_old:\n",
    "        kunk_id_base = vocab_old.index(\"k:UNK\")\n",
    "        parents = parents[parents != kunk_id_base]\n",
    "\n",
    "    if parents.size == 0:\n",
    "        print(f\"[WARN] No parents found at rank {parent_rank} for {r_char}:UNK\")\n",
    "        continue\n",
    "\n",
    "    M_new[parents, unk_id] = 1\n",
    "\n",
    "# 2) Chain UNKs: k:UNK → p:UNK → ... → s:UNK\n",
    "#    This uses the ids from vocab_new.\n",
    "chain = [\"k\",\"p\",\"c\",\"o\",\"f\",\"g\",\"s\"]\n",
    "for a, b in zip(chain[:-1], chain[1:]):\n",
    "    M_new[unk_id_by_rank[a], unk_id_by_rank[b]] = 1\n",
    "\n",
    "# 3) Closure: self-descendants\n",
    "np.fill_diagonal(M_new, 1)\n",
    "\n",
    "# --------------------------\n",
    "# Save artifacts\n",
    "# --------------------------\n",
    "np.save(NEW_M_PATH, M_new)\n",
    "np.save(RANK_IDX_PATH, rank_idx_new)\n",
    "\n",
    "print(\"Saved UNK-extended hierarchy artifacts:\")\n",
    "print(\" -\", NEW_VOCAB_PATH)\n",
    "print(\" -\", NEW_M_PATH)\n",
    "print(\" -\", RANK_IDX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa11326b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates in base vocab: {}\n",
      "k:UNK count in base vocab: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(vocab_old)\n",
    "dups = {k:v for k,v in c.items() if v>1}\n",
    "print(\"Duplicates in base vocab:\", dups)\n",
    "print(\"k:UNK count in base vocab:\", c.get(\"k:UNK\", 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "421f930d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK tokens:\n",
      "6928: k:UNK\n",
      "6929: p:UNK\n",
      "6930: c:UNK\n",
      "6931: o:UNK\n",
      "6932: f:UNK\n",
      "6933: g:UNK\n",
      "6934: s:UNK\n",
      "\n",
      "7x7 descendant matrix for UNK tokens (rows=ancestors, cols=descendants):\n",
      "[[1 1 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [0 0 1 1 0 0 0]\n",
      " [0 0 0 1 1 0 0]\n",
      " [0 0 0 0 1 1 0]\n",
      " [0 0 0 0 0 1 1]\n",
      " [0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Load the extended vocab and matrix\n",
    "vocab_new = json.load(open(NEW_VOCAB_PATH))\n",
    "M_new = np.load(NEW_M_PATH)\n",
    "\n",
    "T_new = len(vocab_new)\n",
    "T_old = T_new - 7   # since last 7 tokens are the UNKs\n",
    "\n",
    "# Extract the 7×7 UNK submatrix\n",
    "M_unk = M_new[T_old:T_new, T_old:T_new]\n",
    "\n",
    "print(\"UNK tokens:\")\n",
    "for i, name in enumerate(vocab_new[T_old:T_new]):\n",
    "    print(f\"{T_old+i}: {name}\")\n",
    "\n",
    "print(\"\\n7x7 descendant matrix for UNK tokens (rows=ancestors, cols=descendants):\")\n",
    "print(M_unk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c091f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_old = 6928, T_new = 6935\n",
      "Index of 'p:UNK' = 6929\n",
      "\n",
      "Parents of 'p:UNK' (rows where M_new[parent, 'p:UNK'] == 1):\n",
      "    3494  k:Archaea\n",
      "    3495  k:Bacteria\n",
      "    6928  k:UNK\n",
      "    6929  p:UNK\n",
      "\n",
      "Check specific expected parents:\n",
      "  k:Archaea   idx=  3494  -> p:UNK edge: True\n",
      "  k:Bacteria  idx=  3495  -> p:UNK edge: True\n",
      "  k:UNK       idx=  6928  -> p:UNK edge: True\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Load vocab + matrix\n",
    "# ----------------------------\n",
    "vocab_new = json.load(open(NEW_VOCAB_PATH))\n",
    "M_new = np.load(NEW_M_PATH)\n",
    "\n",
    "T_new = len(vocab_new)\n",
    "T_old = T_new - 7   # assuming last 7 tokens are the UNKs\n",
    "\n",
    "print(f\"T_old = {T_old}, T_new = {T_new}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Find index of p:UNK\n",
    "# ----------------------------\n",
    "try:\n",
    "    idx_p_unk = vocab_new.index(\"p:UNK\")\n",
    "except ValueError:\n",
    "    raise ValueError(\"'p:UNK' not found in vocab_new. Check naming.\")\n",
    "\n",
    "print(f\"Index of 'p:UNK' = {idx_p_unk}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Find all parents of p:UNK\n",
    "# (rows where M_new[parent, idx_p_unk] == 1)\n",
    "# ----------------------------\n",
    "parents_idx = np.where(M_new[:, idx_p_unk] == 1)[0]\n",
    "\n",
    "print(\"\\nParents of 'p:UNK' (rows where M_new[parent, 'p:UNK'] == 1):\")\n",
    "for p in parents_idx:\n",
    "    print(f\"  {p:6d}  {vocab_new[p]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# check specifically Archaea, Bacteria, k:UNK\n",
    "# ----------------------------\n",
    "names_to_check = [\"k:Archaea\", \"k:Bacteria\", \"k:UNK\"]\n",
    "print(\"\\nCheck specific expected parents:\")\n",
    "for name in names_to_check:\n",
    "    if name in vocab_new:\n",
    "        idx = vocab_new.index(name)\n",
    "        connected = bool(M_new[idx, idx_p_unk] == 1)\n",
    "        print(f\"  {name:10s}  idx={idx:6d}  -> p:UNK edge: {connected}\")\n",
    "    else:\n",
    "        print(f\"  {name:10s}  NOT FOUND in vocab_new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1afaf63",
   "metadata": {},
   "source": [
    "# creating the decendent matrix for the genus corrupted version of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f1502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b85afb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_parent_children_from_nested(taxonomy_nested, name2idx):\n",
    "    \"\"\"\n",
    "    Build parent pointers and children adjacency using taxonomy_nested.json.\n",
    "\n",
    "    parent[i] = parent index of node i, or -1 if i is a top-level clade\n",
    "    children[i] = list of child indices\n",
    "    \"\"\"\n",
    "    N = len(name2idx)\n",
    "    parent = np.full(N, -1, dtype=np.int32)\n",
    "    children = [[] for _ in range(N)]\n",
    "\n",
    "    def dfs(subtree: dict, parent_name=None):\n",
    "        for node_name, child_dict in subtree.items():\n",
    "            if node_name not in name2idx:\n",
    "                continue\n",
    "\n",
    "            i = name2idx[node_name]\n",
    "\n",
    "            if parent_name is not None and parent_name in name2idx:\n",
    "                p = name2idx[parent_name]\n",
    "                parent[i] = p\n",
    "                children[p].append(i)\n",
    "\n",
    "            if isinstance(child_dict, dict) and child_dict:\n",
    "                dfs(child_dict, parent_name=node_name)\n",
    "\n",
    "    dfs(taxonomy_nested, parent_name=None)\n",
    "    return parent, children\n",
    "\n",
    "def compute_depth(children, roots):\n",
    "    \"\"\"\n",
    "    BFS depth from each root.\n",
    "    Any unreachable nodes (e.g., appended k:UNK) are assigned depth=0\n",
    "    and treated as singleton components.\n",
    "    \"\"\"\n",
    "    N = len(children)\n",
    "    depth = np.full(N, -1, dtype=np.int32)\n",
    "\n",
    "    q = deque()\n",
    "    for r in roots:\n",
    "        depth[r] = 0\n",
    "        q.append(r)\n",
    "\n",
    "    while q:\n",
    "        u = q.popleft()\n",
    "        for v in children[u]:\n",
    "            depth[v] = depth[u] + 1\n",
    "            q.append(v)\n",
    "\n",
    "    bad = np.where(depth < 0)[0]\n",
    "    if bad.size > 0:\n",
    "        depth[bad] = 0\n",
    "\n",
    "    return depth\n",
    "\n",
    "\n",
    "def euler_tour_times(children, roots):\n",
    "    \"\"\"\n",
    "    Compute entry/exit times (tin/tout) for each node.\n",
    "    Unreachable nodes (e.g., appended k:UNK) get singleton intervals.\n",
    "    \"\"\"\n",
    "    N = len(children)\n",
    "    tin = np.full(N, -1, dtype=np.int32)\n",
    "    tout = np.full(N, -1, dtype=np.int32)\n",
    "    t = 0\n",
    "\n",
    "    def dfs(u):\n",
    "        nonlocal t\n",
    "        tin[u] = t\n",
    "        t += 1\n",
    "        for v in children[u]:\n",
    "            dfs(v)\n",
    "        tout[u] = t\n",
    "        t += 1\n",
    "\n",
    "    for r in roots:\n",
    "        if tin[r] == -1:\n",
    "            dfs(r)\n",
    "\n",
    "    bad = np.where(tin < 0)[0]\n",
    "    for u in bad:\n",
    "        tin[u] = t\n",
    "        t += 1\n",
    "        tout[u] = t\n",
    "        t += 1\n",
    "\n",
    "    return tin, tout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c87779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_DIR_GENUS = /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_genus_silva_sintax/dataset_full_top999\n",
      "TREE_DIR          = /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_genus_silva_sintax/dataset_full_top999/tree_artifacts\n",
      "N taxonomy nodes = 6115\n",
      "Top-level clades: ['k:Bacteria', 'k:Archaea']\n",
      "Number of roots (top-level clades) = 2\n",
      "First 10 roots: ['k:Archaea', 'k:Bacteria']\n",
      "Depth min/max: 0 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building descendant matrix (dense): 100%|██████████| 6115/6115 [00:00<00:00, 141249.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descendant matrix shape: (6115, 6115) dtype: uint8\n",
      "Diagonal sum (should be N): 6115\n",
      "Nonzero entries: 36501\n",
      "Saved: /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_genus_silva_sintax/dataset_full_top999/tree_artifacts/descendant_matrix.npy\n",
      "Saved: /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_genus_silva_sintax/dataset_full_top999/tree_artifacts/README_descendants.md\n",
      "Loaded base taxonomy: T_old=6115\n",
      "Extended vocab saved: T_new=6121 (added 6 UNKs)\n",
      "UNKs added: ['p:UNK', 'c:UNK', 'o:UNK', 'f:UNK', 'g:UNK', 's:UNK']\n",
      "Saved UNK-extended hierarchy artifacts:\n",
      " - /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_genus_silva_sintax/dataset_full_top999/tree_artifacts/taxonomy_vocab_with_unk.json\n",
      " - /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_genus_silva_sintax/dataset_full_top999/tree_artifacts/descendant_matrix_with_unk.npy\n",
      " - /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_genus_silva_sintax/dataset_full_top999/tree_artifacts/rank_idx.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------------------\n",
    "# Paths (NEW dataset)\n",
    "# --------------------------\n",
    "DATASET_DIR_GENUS = Path(\n",
    "    \"/home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training\"\n",
    "    \"/level_97/silva-138.2/incomplete_genus_silva_sintax/dataset_full_top999\"\n",
    ")\n",
    "\n",
    "VOCAB_PATH  = DATASET_DIR_GENUS / \"taxonomy_vocab.json\"\n",
    "NESTED_PATH = DATASET_DIR_GENUS / \"taxonomy_nested.json\"\n",
    "\n",
    "assert VOCAB_PATH.exists(), f\"Missing: {VOCAB_PATH}\"\n",
    "assert NESTED_PATH.exists(), f\"Missing: {NESTED_PATH}\"\n",
    "\n",
    "TREE_DIR = DATASET_DIR_GENUS / \"tree_artifacts\"\n",
    "TREE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATASET_DIR_GENUS =\", DATASET_DIR_GENUS)\n",
    "print(\"TREE_DIR          =\", TREE_DIR)\n",
    "\n",
    "# --------------------------\n",
    "# Load vocab + nested tree\n",
    "# --------------------------\n",
    "names = json.load(open(VOCAB_PATH, \"r\"))\n",
    "name2idx = {nm: i for i, nm in enumerate(names)}\n",
    "N = len(names)\n",
    "\n",
    "taxonomy_nested = json.load(open(NESTED_PATH, \"r\"))\n",
    "\n",
    "print(\"N taxonomy nodes =\", N)\n",
    "print(\"Top-level clades:\", list(taxonomy_nested.keys())[:10])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Build parent pointers and children adjacency from nested dict\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "parent, children = build_parent_children_from_nested(taxonomy_nested, name2idx)\n",
    "roots = [i for i in np.where(parent == -1)[0].tolist() if names[i] in taxonomy_nested]\n",
    "\n",
    "print(\"Number of roots (top-level clades) =\", len(roots))\n",
    "print(\"First 10 roots:\", [names[i] for i in roots[:10]])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Compute depth (optional but convenient for debugging)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "depth = compute_depth(children, roots)\n",
    "print(\"Depth min/max:\", int(depth.min()), int(depth.max()))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Euler tour times + dense descendant-closure matrix\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "tin, tout = euler_tour_times(children, roots)\n",
    "\n",
    "def build_descendant_matrix_dense(tin, tout, dtype=np.uint8):\n",
    "    \"\"\"\n",
    "    Dense descendant-closure matrix:\n",
    "      M[i, j] = 1 iff i is ancestor of j (including i==j)\n",
    "    \"\"\"\n",
    "    N = len(tin)\n",
    "    M = np.zeros((N, N), dtype=dtype)\n",
    "    for i in tqdm(range(N), desc=\"Building descendant matrix (dense)\"):\n",
    "        M[i, :] = ((tin[i] <= tin) & (tout <= tout[i])).astype(dtype)\n",
    "    return M\n",
    "\n",
    "M = build_descendant_matrix_dense(tin, tout, dtype=np.uint8)\n",
    "\n",
    "print(\"Descendant matrix shape:\", M.shape, \"dtype:\", M.dtype)\n",
    "print(\"Diagonal sum (should be N):\", int(np.trace(M)))\n",
    "print(\"Nonzero entries:\", int(M.sum()))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Save base tree artifacts (GENUS dataset)\n",
    "# ============================================================\n",
    "\n",
    "np.save(TREE_DIR / \"parent.npy\", parent)\n",
    "np.save(TREE_DIR / \"depth.npy\", depth)\n",
    "np.save(TREE_DIR / \"tin.npy\", tin)\n",
    "np.save(TREE_DIR / \"tout.npy\", tout)\n",
    "\n",
    "desc_path = TREE_DIR / \"descendant_matrix.npy\"\n",
    "np.save(desc_path, M)\n",
    "print(\"Saved:\", desc_path)\n",
    "\n",
    "readme_path = TREE_DIR / \"README_descendants.md\"\n",
    "with open(readme_path, \"w\") as f:\n",
    "    f.write(\n",
    "        \"# Descendant matrix (ancestor/descendant closure)\\n\\n\"\n",
    "        \"Files in this folder are derived from taxonomy_nested.json and taxonomy_vocab.json.\\n\\n\"\n",
    "        \"## descendant_matrix.npy\\n\"\n",
    "        \"Let `names = taxonomy_vocab.json` (length N).\\n\"\n",
    "        \"The matrix `M` has shape [N, N] and is defined as:\\n\"\n",
    "        \"`M[i, j] = 1` iff node `names[j]` is equal to or a descendant of node `names[i]`.\\n\\n\"\n",
    "        \"This matches the old ETE3 implementation, but uses an Euler tour (tin/tout) for speed.\\n\"\n",
    "    )\n",
    "print(\"Saved:\", readme_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) UNK-extend vocab + descendant matrix + rank_idx (GENUS dataset)\n",
    "#     (matches your existing logic)\n",
    "# ============================================================\n",
    "\n",
    "BASE_VOCAB_PATH = DATASET_DIR_GENUS / \"taxonomy_vocab.json\"\n",
    "BASE_M_PATH     = TREE_DIR / \"descendant_matrix.npy\"\n",
    "\n",
    "NEW_VOCAB_PATH  = TREE_DIR / \"taxonomy_vocab_with_unk.json\"\n",
    "NEW_M_PATH      = TREE_DIR / \"descendant_matrix_with_unk.npy\"\n",
    "RANK_IDX_PATH   = TREE_DIR / \"rank_idx.npy\"\n",
    "\n",
    "with open(BASE_VOCAB_PATH, \"r\") as f:\n",
    "    vocab_old = json.load(f)\n",
    "\n",
    "M_old = np.load(BASE_M_PATH)\n",
    "T_old = len(vocab_old)\n",
    "assert M_old.shape == (T_old, T_old), \"Mismatch between vocab and descendant matrix\"\n",
    "print(f\"Loaded base taxonomy: T_old={T_old}\")\n",
    "\n",
    "RANK_CHAR_TO_IDX = {\"k\":0, \"p\":1, \"c\":2, \"o\":3, \"f\":4, \"g\":5, \"s\":6}\n",
    "\n",
    "rank_idx_old = np.empty(T_old, dtype=np.int64)\n",
    "for i, name in enumerate(vocab_old):\n",
    "    c = name[0].lower()\n",
    "    if c not in RANK_CHAR_TO_IDX:\n",
    "        raise ValueError(f\"Cannot infer rank from token: {name}\")\n",
    "    rank_idx_old[i] = RANK_CHAR_TO_IDX[c]\n",
    "\n",
    "desired_unk = [f\"{r}:UNK\" for r in [\"k\",\"p\",\"c\",\"o\",\"f\",\"g\",\"s\"]]\n",
    "have = set(vocab_old)\n",
    "\n",
    "unk_tokens_to_add = [tok for tok in desired_unk if tok not in have]\n",
    "vocab_new = vocab_old + unk_tokens_to_add\n",
    "T_new = len(vocab_new)\n",
    "\n",
    "with open(NEW_VOCAB_PATH, \"w\") as f:\n",
    "    json.dump(vocab_new, f, indent=2)\n",
    "\n",
    "print(f\"Extended vocab saved: T_new={T_new} (added {len(unk_tokens_to_add)} UNKs)\")\n",
    "print(\"UNKs added:\", unk_tokens_to_add)\n",
    "\n",
    "rank_idx_new = np.empty(T_new, dtype=np.int64)\n",
    "rank_idx_new[:T_old] = rank_idx_old\n",
    "for j, tok in enumerate(unk_tokens_to_add, start=T_old):\n",
    "    rank_idx_new[j] = RANK_CHAR_TO_IDX[tok[0].lower()]\n",
    "\n",
    "M_new = np.zeros((T_new, T_new), dtype=M_old.dtype)\n",
    "M_new[:T_old, :T_old] = M_old\n",
    "\n",
    "unk_id_by_rank = {}\n",
    "for tok in desired_unk:\n",
    "    idx = vocab_new.index(tok)\n",
    "    unk_id_by_rank[tok[0].lower()] = idx\n",
    "\n",
    "# Link UNK_r (r>=1) as child of ALL base nodes at rank r-1\n",
    "for r_char, r in [(\"p\",1), (\"c\",2), (\"o\",3), (\"f\",4), (\"g\",5), (\"s\",6)]:\n",
    "    unk_id = unk_id_by_rank[r_char]\n",
    "    parent_rank = r - 1\n",
    "\n",
    "    parents = np.where(rank_idx_old == parent_rank)[0]\n",
    "\n",
    "    # exclude k:UNK from being a parent of p:UNK (if present in base)\n",
    "    if parent_rank == 0 and \"k:UNK\" in vocab_old:\n",
    "        kunk_id_base = vocab_old.index(\"k:UNK\")\n",
    "        parents = parents[parents != kunk_id_base]\n",
    "\n",
    "    if parents.size == 0:\n",
    "        print(f\"[WARN] No parents found at rank {parent_rank} for {r_char}:UNK\")\n",
    "        continue\n",
    "\n",
    "    M_new[parents, unk_id] = 1\n",
    "\n",
    "# Chain UNKs: k:UNK → p:UNK → ... → s:UNK\n",
    "chain = [\"k\",\"p\",\"c\",\"o\",\"f\",\"g\",\"s\"]\n",
    "for a, b in zip(chain[:-1], chain[1:]):\n",
    "    M_new[unk_id_by_rank[a], unk_id_by_rank[b]] = 1\n",
    "\n",
    "np.fill_diagonal(M_new, 1)\n",
    "\n",
    "np.save(NEW_M_PATH, M_new)\n",
    "np.save(RANK_IDX_PATH, rank_idx_new)\n",
    "\n",
    "print(\"Saved UNK-extended hierarchy artifacts:\")\n",
    "print(\" -\", NEW_VOCAB_PATH)\n",
    "print(\" -\", NEW_M_PATH)\n",
    "print(\" -\", RANK_IDX_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35351edb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_ontology_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

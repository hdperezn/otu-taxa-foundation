{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330b9c49",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Microbe Atlas preprocessing for OTU+Taxa foundation training\n",
    "# ============================================================\n",
    "Notebook: otu-taxa-foundation/notebooks/preprocess/01_explore_raw_data.ipynb\n",
    "\n",
    "Output root (processed data is NOT stored inside the repo):\n",
    "/home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c538b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Optional, Dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f5d79",
   "metadata": {},
   "source": [
    "# ----------------------------\n",
    "# config\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef16ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned output directory:\n",
      " /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_silva_sintax/dataset_full_top999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CFG = {\n",
    "    # ---- raw inputs ----\n",
    "    \"filtered_counts_path\": \"/home/hernan_melmoth/Documents/phd_work/Bio_ontology/MicrobeAtlas/level_97/samples-otus-97.filtered-5-reads.json\",\n",
    "    \"pred_sintax_path\": \"/home/hernan_melmoth/Documents/phd_work/Bio_ontology/MicrobeAtlas/level_97/taxonomy_reference/silva-138.2/vsearch_incomplete_species_fromOTUS_predictions/repseqs_sintax_incomplete.txt\",\n",
    "\n",
    "    # ---- preprocessing policy ----\n",
    "    \"keep_fraction\": 0.999,  # top-99.9% cumulative abundance\n",
    "    \"taxonomy_policy\": \"contiguous_valid_prefix\",  # matches your current logic\n",
    "\n",
    "    # ---- processed data root (outside repo) ----\n",
    "    \"processed_root\": \"/home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training\",\n",
    "\n",
    "    # ---- naming tags for clarity in folder structure ----\n",
    "    \"level_tag\": \"level_97\",\n",
    "    \"silva_tag\": \"silva-138.2\",\n",
    "    \"taxonomy_tag\": \"incomplete_silva_sintax\",  # training version\n",
    "    \"dataset_tag\": \"dataset_full_top999\",       # describes scope + policy\n",
    "    }\n",
    "CFG[\"append_unk_kingdom\"] = True\n",
    "CFG[\"unk_kingdom_token\"] = \"k:UNK\"\n",
    "\n",
    "OUT_DIR = os.path.join(\n",
    "    CFG[\"processed_root\"],\n",
    "    CFG[\"level_tag\"],\n",
    "    CFG[\"silva_tag\"],\n",
    "    CFG[\"taxonomy_tag\"],\n",
    "    CFG[\"dataset_tag\"],\n",
    ")\n",
    "\n",
    "print(\"Planned output directory:\\n\", OUT_DIR)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f3e032",
   "metadata": {},
   "source": [
    "# ----------------------------\n",
    "# Loaders functions\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acf3f22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111870, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>otu_id</th>\n",
       "      <th>taxonomy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90_19327;96_77520;97_100055</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90_18588;96_76070;97_104571</td>\n",
       "      <td>k:Bacteria,p:Cyanobacteria,c:Chloroplast,o:Sol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90_22156;96_86043;97_110485</td>\n",
       "      <td>k:Bacteria,p:Cyanobacteria,c:Chloroplast,o:Sol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90_20463;96_79800;97_102794</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90_17477;96_14804;97_18077</td>\n",
       "      <td>k:Bacteria,p:Bacteroidetes,c:Cytophagia,o:Cyto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        otu_id  \\\n",
       "0  90_19327;96_77520;97_100055   \n",
       "1  90_18588;96_76070;97_104571   \n",
       "2  90_22156;96_86043;97_110485   \n",
       "3  90_20463;96_79800;97_102794   \n",
       "4   90_17477;96_14804;97_18077   \n",
       "\n",
       "                                            taxonomy  \n",
       "0                                                     \n",
       "1  k:Bacteria,p:Cyanobacteria,c:Chloroplast,o:Sol...  \n",
       "2  k:Bacteria,p:Cyanobacteria,c:Chloroplast,o:Sol...  \n",
       "3                                                     \n",
       "4  k:Bacteria,p:Bacteroidetes,c:Cytophagia,o:Cyto...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_sintax_table(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load SINTAX output into a 2-column dataframe:\n",
    "      otu_id | taxonomy\n",
    "\n",
    "    Tries the (0,3) column layout first; falls back to parsing raw strings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", header=None, engine=\"python\",\n",
    "                         usecols=[0, 3], names=[\"otu_id\", \"taxonomy\"], dtype=str)\n",
    "    except Exception:\n",
    "        df_raw = pd.read_csv(path, sep=\"\\t\", header=None, engine=\"python\",\n",
    "                             usecols=[0, 1], names=[\"otu_id\", \"raw_sintax\"], dtype=str)\n",
    "\n",
    "        def drop_conf(s: str) -> str:\n",
    "            if pd.isna(s): \n",
    "                return \"\"\n",
    "            parts = []\n",
    "            for p in s.strip().rstrip(\";\").split(\",\"):\n",
    "                if \":\" not in p:\n",
    "                    continue\n",
    "                parts.append(p.split(\"(\", 1)[0].strip())\n",
    "            return \",\".join(parts)\n",
    "\n",
    "        df_raw[\"taxonomy\"] = df_raw[\"raw_sintax\"].apply(drop_conf)\n",
    "        df = df_raw[[\"otu_id\", \"taxonomy\"]]\n",
    "\n",
    "    df[\"otu_id\"] = df[\"otu_id\"].astype(str)\n",
    "    df[\"taxonomy\"] = df[\"taxonomy\"].fillna(\"\").astype(str)\n",
    "    return df\n",
    "\n",
    "pred_df = load_sintax_table(CFG[\"pred_sintax_path\"])\n",
    "print(pred_df.shape)\n",
    "pred_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2ea609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Taxonomy parsing: enforce contiguous + valid prefix\n",
    "# ============================================================\n",
    "\n",
    "RANKS: List[str] = [\"k\", \"p\", \"c\", \"o\", \"f\", \"g\", \"s\"]\n",
    "RANK_TO_IDX: Dict[str, int] = {r: i for i, r in enumerate(RANKS)}\n",
    "_CONF_TAIL_RE = re.compile(r\"\\s*\\([^)]*\\)\\s*$\")\n",
    "\n",
    "def strip_confidence(name: str) -> str:\n",
    "    if not isinstance(name, str):\n",
    "        return \"\"\n",
    "    return _CONF_TAIL_RE.sub(\"\", name).strip().rstrip(\";\").strip()\n",
    "\n",
    "def split_tax_path(tax_str: str) -> List[str]:\n",
    "    if not isinstance(tax_str, str) or not tax_str:\n",
    "        return []\n",
    "    s = tax_str.strip().rstrip(\";\").replace(\";\", \",\")\n",
    "    return [p.strip() for p in s.split(\",\") if p.strip()]\n",
    "\n",
    "def parse_token(tok: str) -> Tuple[Optional[str], str]:\n",
    "    if not isinstance(tok, str) or \":\" not in tok:\n",
    "        return None, \"\"\n",
    "    r, name = tok.split(\":\", 1)\n",
    "    r = (r or \"\").strip().lower()\n",
    "    name = strip_confidence(name)\n",
    "    if r not in RANKS:\n",
    "        return None, name\n",
    "    return r, name\n",
    "\n",
    "def is_unidentified_name(name: str) -> bool:\n",
    "    n = (name or \"\").strip().strip(\"'\\\"\").lower()\n",
    "    return n in {\"unidentified\", \"unknown\", \"__unknown\"}\n",
    "\n",
    "def is_valid_token(tok: str) -> bool:\n",
    "    r, name = parse_token(tok)\n",
    "    if r is None:\n",
    "        return False\n",
    "    if name == \"\":\n",
    "        return False\n",
    "    if is_unidentified_name(name):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def pick_chain(tokens: List[str]) -> List[Optional[str]]:\n",
    "    chain: List[Optional[str]] = []\n",
    "    start = 0\n",
    "    for r in RANKS:\n",
    "        pref = r + \":\"\n",
    "        found = None\n",
    "        for i in range(start, len(tokens)):\n",
    "            t = tokens[i]\n",
    "            if isinstance(t, str) and t.startswith(pref):\n",
    "                found = (i, t)\n",
    "                break\n",
    "        if found is None:\n",
    "            chain.append(None)\n",
    "        else:\n",
    "            chain.append(found[1])\n",
    "            start = found[0] + 1\n",
    "    return chain\n",
    "\n",
    "def last_contiguous_valid_token(tokens: List[str]) -> Optional[str]:\n",
    "    chain = pick_chain(tokens)\n",
    "    last_valid = None\n",
    "    for t in chain:\n",
    "        if t is None or not is_valid_token(t):\n",
    "            break\n",
    "        last_valid = t\n",
    "    return last_valid\n",
    "\n",
    "def contiguous_chain(tokens: List[str]) -> List[str]:\n",
    "    chain = pick_chain(tokens)\n",
    "    out: List[str] = []\n",
    "    for t in chain:\n",
    "        if t is None or not is_valid_token(t):\n",
    "            break\n",
    "        out.append(t)\n",
    "    return out\n",
    "\n",
    "def token_depth(tok: str) -> Optional[int]:\n",
    "    r, _ = parse_token(tok)\n",
    "    return None if r is None else RANK_TO_IDX[r]\n",
    "\n",
    "def ensure_child(mapping: dict, key: str) -> dict:\n",
    "    if key not in mapping:\n",
    "        mapping[key] = {}\n",
    "    return mapping[key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60bfeb1",
   "metadata": {},
   "source": [
    "# Build Corpus fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a537be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Dataset builder: full corpus, top-abundance OTUs, keep OTUs\n",
    "# without usable taxonomy but encode with appended k:UNK id.\n",
    "# ============================================================\n",
    "\n",
    "def build_dataset_full_corpus(\n",
    "    *,\n",
    "    counts_by_sample: Dict[str, Dict[str, float]],\n",
    "    pred_df: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    keep_fraction: float = 0.999,\n",
    "    unk_kingdom_token: str = \"k:UNK\",\n",
    "    append_unk_kingdom: bool = True,\n",
    "    sanity_check_samples: int = 2000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build Microbe Atlas training dataset (full corpus) with the artifact contract:\n",
    "\n",
    "      - otu_vocab.json        (list[str])\n",
    "      - taxonomy_vocab.json   (list[str] of ALL tree nodes induced by valid contiguous prefixes,\n",
    "                              plus optionally one appended UNK kingdom token at the end)\n",
    "      - samples.jsonl         (one JSON per sample: {sample_id, otus:[int], taxa:[int]})\n",
    "                              taxonomy ids are NEVER -1; missing taxonomy => unk_k_id\n",
    "      - taxonomy_nested.json  (inspection-only nested dict)\n",
    "      - dropped_otus.json     (OTUs with missing/unusable taxonomy; they are KEPT)\n",
    "\n",
    "    Taxonomy policy:\n",
    "      - Parse SINTAX taxonomy string\n",
    "      - Take the contiguous VALID prefix in rank order (k→…→s)\n",
    "      - Label = last token of that prefix (deepest contiguous valid token)\n",
    "      - Nodes in taxonomy_vocab = union of all tokens in the contiguous valid prefixes\n",
    "      - OTUs with no usable taxonomy are kept; their sample positions use unk_k_id\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 1) Aggregate OTU totals over ALL samples\n",
    "    # ---------------------------------------\n",
    "    all_sample_ids = list(counts_by_sample.keys())\n",
    "    print(f\"[INFO] Total samples (full corpus): {len(all_sample_ids)}\")\n",
    "\n",
    "    otu_set = set()\n",
    "    for sid in all_sample_ids:\n",
    "        otu_set.update(counts_by_sample[sid].keys())\n",
    "\n",
    "    otu_list_all = sorted(otu_set)\n",
    "    otu2idx_all = {otu: i for i, otu in enumerate(otu_list_all)}\n",
    "    O_all = len(otu_list_all)\n",
    "    print(f\"[INFO] Global OTU vocab (pre-cut): {O_all}\")\n",
    "\n",
    "    totals = np.zeros(O_all, dtype=np.float64)\n",
    "    for sid in tqdm(all_sample_ids, desc=\"Accumulating OTU totals\"):\n",
    "        for otu, cnt in counts_by_sample[sid].items():\n",
    "            totals[otu2idx_all[otu]] += float(cnt)\n",
    "\n",
    "    order_desc = np.argsort(-totals)\n",
    "    cum = np.cumsum(totals[order_desc])\n",
    "    total_sum = float(cum[-1]) if cum.size else 0.0\n",
    "    target = keep_fraction * total_sum\n",
    "    k_keep = int(np.searchsorted(cum, target)) + 1\n",
    "\n",
    "    kept_idx = order_desc[:k_keep]\n",
    "    kept_otu_list = [otu_list_all[i] for i in kept_idx]\n",
    "    print(f\"[INFO] Kept OTUs (top {keep_fraction:.3%} cumulative abundance): {len(kept_otu_list)}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2) Build OTU -> taxonomy mapping (where possible) and collect ALL nodes\n",
    "    # --------------------------------------------------\n",
    "    pred_df2 = pred_df.drop_duplicates(\"otu_id\")\n",
    "    otu_to_taxstr = dict(zip(pred_df2[\"otu_id\"].astype(str), pred_df2[\"taxonomy\"].fillna(\"\").astype(str)))\n",
    "\n",
    "    taxonomy_nodes = set()\n",
    "    otu2tax: Dict[str, str] = {}  # only OTUs with valid contiguous label\n",
    "    n_truncated = 0\n",
    "    depth_counter = Counter()\n",
    "\n",
    "    for otu in tqdm(kept_otu_list, desc=\"Mapping OTUs to taxonomy\"):\n",
    "        tstr = otu_to_taxstr.get(otu, \"\")\n",
    "        toks = split_tax_path(tstr)\n",
    "        if not toks:\n",
    "            continue\n",
    "\n",
    "        lc = last_contiguous_valid_token(toks)\n",
    "        if lc is None:\n",
    "            continue\n",
    "\n",
    "        # contiguity truncation indicator\n",
    "        if lc != toks[-1]:\n",
    "            n_truncated += 1\n",
    "\n",
    "        otu2tax[otu] = lc\n",
    "\n",
    "        d = token_depth(lc)\n",
    "        if d is not None:\n",
    "            depth_counter[RANKS[d]] += 1\n",
    "\n",
    "        # collect ALL nodes from the contiguous valid prefix\n",
    "        prefix = contiguous_chain(toks)\n",
    "        for tok in prefix:\n",
    "            taxonomy_nodes.add(tok)\n",
    "\n",
    "    print(f\"[INFO] Contiguity/invalid truncations applied to: {n_truncated}\")\n",
    "    print(f\"[INFO] Label depth distribution: {dict(depth_counter)}\")\n",
    "\n",
    "    # -----------------------\n",
    "    # 3) Record OTUs missing taxonomy (but DO NOT drop)\n",
    "    # -----------------------\n",
    "    dropped_otus = [otu for otu in kept_otu_list if otu not in otu2tax]\n",
    "    print(f\"[INFO] OTUs with missing/unusable taxonomy (kept): {len(dropped_otus)}\")\n",
    "\n",
    "    # -----------------------\n",
    "    # 4) Build vocabularies\n",
    "    # -----------------------\n",
    "    taxonomy_vocab = sorted(taxonomy_nodes)  # base nodes only\n",
    "\n",
    "    unk_k_id = None\n",
    "    if append_unk_kingdom:\n",
    "        # enforce \"UNK kingdom token is last\"\n",
    "        if unk_kingdom_token in taxonomy_vocab:\n",
    "            taxonomy_vocab = [t for t in taxonomy_vocab if t != unk_kingdom_token]\n",
    "        taxonomy_vocab.append(unk_kingdom_token)\n",
    "        unk_k_id = len(taxonomy_vocab) - 1\n",
    "\n",
    "    tax2idx = {t: i for i, t in enumerate(taxonomy_vocab)}\n",
    "    otu2idx_kept = {otu: i for i, otu in enumerate(kept_otu_list)}  # includes missing-tax OTUs\n",
    "\n",
    "    print(f\"[INFO] taxonomy_vocab size: {len(taxonomy_vocab)} (append_unk_kingdom={append_unk_kingdom})\")\n",
    "    if append_unk_kingdom:\n",
    "        print(f\"[INFO] UNK kingdom token id: {unk_k_id} ({taxonomy_vocab[unk_k_id]})\")\n",
    "    print(f\"[INFO] otu_vocab size (kept): {len(kept_otu_list)}\")\n",
    "\n",
    "    # -----------------------\n",
    "    # 5) Build nested taxonomy (inspection only)\n",
    "    #     Only from OTUs that have a valid contiguous prefix.\n",
    "    # -----------------------\n",
    "    taxonomy_nested = {}\n",
    "    for otu in tqdm(kept_otu_list, desc=\"Building taxonomy_nested\"):\n",
    "        tstr = otu_to_taxstr.get(otu, \"\")\n",
    "        toks = split_tax_path(tstr)\n",
    "        prefix = contiguous_chain(toks)\n",
    "        if not prefix:\n",
    "            continue\n",
    "        cur = taxonomy_nested\n",
    "        for tok in prefix:\n",
    "            cur = ensure_child(cur, tok)\n",
    "\n",
    "    # -----------------------\n",
    "    # 6) Write artifacts\n",
    "    # -----------------------\n",
    "    with open(os.path.join(out_dir, \"otu_vocab.json\"), \"w\") as f:\n",
    "        json.dump(kept_otu_list, f)\n",
    "\n",
    "    with open(os.path.join(out_dir, \"taxonomy_vocab.json\"), \"w\") as f:\n",
    "        json.dump(taxonomy_vocab, f)\n",
    "\n",
    "    with open(os.path.join(out_dir, \"taxonomy_nested.json\"), \"w\") as f:\n",
    "        json.dump(taxonomy_nested, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    with open(os.path.join(out_dir, \"dropped_otus.json\"), \"w\") as f:\n",
    "        json.dump(dropped_otus, f, indent=2)\n",
    "\n",
    "    # config snapshot (expects CFG in outer scope; if not, remove CFG references)\n",
    "    config_path = os.path.join(out_dir, \"config.json\")\n",
    "    config_obj = {\n",
    "        \"keep_fraction\": keep_fraction,\n",
    "        \"taxonomy_policy\": \"contiguous_valid_prefix\",\n",
    "        \"append_unk_kingdom\": append_unk_kingdom,\n",
    "        \"unk_kingdom_token\": unk_kingdom_token,\n",
    "        \"unk_kingdom_id\": unk_k_id,\n",
    "        \"note\": \"OTUs with missing/unusable taxonomy are kept; their taxa use the appended k:UNK id (last token).\",\n",
    "    }\n",
    "    # If CFG exists in notebook, include raw paths as well\n",
    "    if \"CFG\" in globals():\n",
    "        config_obj.update({\n",
    "            \"filtered_counts_path\": CFG.get(\"filtered_counts_path\"),\n",
    "            \"pred_sintax_path\": CFG.get(\"pred_sintax_path\"),\n",
    "        })\n",
    "\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(config_obj, f, indent=2)\n",
    "\n",
    "    # README\n",
    "    readme_path = os.path.join(out_dir, \"README.md\")\n",
    "    with open(readme_path, \"w\") as f:\n",
    "        f.write(\n",
    "            \"# Microbe Atlas dataset artifacts (full corpus)\\n\\n\"\n",
    "            \"This folder contains the artifacts consumed by the OTU–Taxa foundation model.\\n\\n\"\n",
    "            \"## Files\\n\"\n",
    "            \"- `otu_vocab.json`: OTU token vocabulary (list of OTU string identifiers)\\n\"\n",
    "            \"- `taxonomy_vocab.json`: taxonomy vocabulary containing all induced tree nodes; \"\n",
    "            + (\"plus an appended `k:UNK` token as the last id\\n\" if append_unk_kingdom else \"\\n\") +\n",
    "            \"- `taxonomy_nested.json`: nested dict for manual inspection (not required by training)\\n\"\n",
    "            \"- `dropped_otus.json`: OTUs with missing/unusable taxonomy (kept in `otu_vocab.json`)\\n\"\n",
    "            \"- `samples.jsonl`: one JSON per sample: `{sample_id, otus:[...], taxa:[...]}`\\n\\n\"\n",
    "            \"Notes:\\n\"\n",
    "            + (f\"- Missing/unusable taxonomy is encoded as the `k:UNK` id (always last): {unk_k_id}\\n\"\n",
    "               if append_unk_kingdom else\n",
    "               \"- Missing/unusable taxonomy is not encoded here (append_unk_kingdom=False).\\n\")\n",
    "        )\n",
    "\n",
    "    # -----------------------\n",
    "    # 7) Write samples.jsonl\n",
    "    #     taxonomy id = real tax_id if available else unk_k_id\n",
    "    # -----------------------\n",
    "    if append_unk_kingdom and unk_k_id is None:\n",
    "        raise RuntimeError(\"append_unk_kingdom=True but unk_k_id is None (unexpected).\")\n",
    "\n",
    "    jsonl_path = os.path.join(out_dir, \"samples.jsonl\")\n",
    "    n_written = 0\n",
    "    n_empty = 0\n",
    "    n_tax_missing_positions = 0\n",
    "\n",
    "    with open(jsonl_path, \"w\") as fout:\n",
    "        for sid in tqdm(all_sample_ids, desc=\"Writing samples.jsonl\"):\n",
    "            row = counts_by_sample[sid]\n",
    "\n",
    "            triplets = []\n",
    "            for otu, cnt in row.items():\n",
    "                j = otu2idx_kept.get(otu)\n",
    "                if j is None:\n",
    "                    continue  # OTU not in kept vocab\n",
    "\n",
    "                tok = otu2tax.get(otu, None)\n",
    "                if tok is None:\n",
    "                    tax_id = unk_k_id  # no -1 anywhere\n",
    "                    n_tax_missing_positions += 1\n",
    "                else:\n",
    "                    tax_id = tax2idx[tok]  # guaranteed present\n",
    "\n",
    "                triplets.append((j, float(cnt), int(tax_id)))\n",
    "\n",
    "            if not triplets:\n",
    "                n_empty += 1\n",
    "                continue\n",
    "\n",
    "            # sort by abundance desc\n",
    "            triplets.sort(key=lambda x: -x[1])\n",
    "            otus_idx = [j for (j, _, _) in triplets]\n",
    "            taxa_idx = [t for (_, _, t) in triplets]\n",
    "\n",
    "            fout.write(json.dumps({\"sample_id\": sid, \"otus\": otus_idx, \"taxa\": taxa_idx}) + \"\\n\")\n",
    "            n_written += 1\n",
    "\n",
    "    print(f\"[SAVE] samples.jsonl: wrote={n_written}, skipped_empty={n_empty}\")\n",
    "    print(f\"[INFO] Missing-taxonomy positions encoded as unk_k_id: {n_tax_missing_positions}\")\n",
    "    print(f\"[OK] out_dir = {out_dir}\")\n",
    "\n",
    "    # -----------------------\n",
    "    # 8) Quick sanity checks\n",
    "    # -----------------------\n",
    "    if append_unk_kingdom:\n",
    "        assert taxonomy_vocab[-1] == unk_kingdom_token, \"UNK token is not last in taxonomy_vocab.\"\n",
    "        assert unk_k_id == len(taxonomy_vocab) - 1, \"UNK id is not the last index.\"\n",
    "\n",
    "    # sample-check that no negative tax ids exist in samples.jsonl\n",
    "    bad = 0\n",
    "    with open(jsonl_path, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            rec = json.loads(line)\n",
    "            if any(int(t) < 0 for t in rec[\"taxa\"]):\n",
    "                bad += 1\n",
    "                if bad <= 3:\n",
    "                    print(\"[BAD] Found negative tax id in sample:\", rec.get(\"sample_id\"))\n",
    "            if i >= max(0, sanity_check_samples - 1):\n",
    "                break\n",
    "    if bad == 0:\n",
    "        print(f\"[CHECK] No negative tax ids found in first {sanity_check_samples} samples.\")\n",
    "    else:\n",
    "        print(f\"[WARNING] Found {bad} samples with negative tax ids in first {sanity_check_samples} samples.\")\n",
    "\n",
    "    return {\n",
    "        \"out_dir\": out_dir,\n",
    "        \"n_samples_total\": len(all_sample_ids),\n",
    "        \"n_samples_written\": n_written,\n",
    "        \"n_samples_empty\": n_empty,\n",
    "        \"n_otus_pre\": O_all,\n",
    "        \"n_otus_kept\": len(kept_otu_list),\n",
    "        \"n_tax_nodes\": len(taxonomy_vocab),\n",
    "        \"n_otus_missing_taxonomy\": len(dropped_otus),\n",
    "        \"n_positions_missing_taxonomy\": n_tax_missing_positions,\n",
    "        \"unk_kingdom_id\": unk_k_id,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b426bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total samples (full corpus): 1836250\n",
      "[INFO] Global OTU vocab (pre-cut): 99335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Accumulating OTU totals: 100%|██████████| 1836250/1836250 [00:59<00:00, 30937.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Kept OTUs (top 99.900% cumulative abundance): 62200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping OTUs to taxonomy: 100%|██████████| 62200/62200 [00:00<00:00, 66334.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Contiguity/invalid truncations applied to: 22\n",
      "[INFO] Label depth distribution: {'g': 27020, 'f': 13080, 'o': 9387, 'c': 3445, 's': 3009, 'p': 2936, 'k': 100}\n",
      "[INFO] OTUs with missing/unusable taxonomy (kept): 3223\n",
      "[INFO] taxonomy_vocab size: 6929 (append_unk_kingdom=True)\n",
      "[INFO] UNK kingdom token id: 6928 (k:UNK)\n",
      "[INFO] otu_vocab size (kept): 62200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building taxonomy_nested: 100%|██████████| 62200/62200 [00:00<00:00, 116121.42it/s]\n",
      "Writing samples.jsonl: 100%|██████████| 1836250/1836250 [02:26<00:00, 12521.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] samples.jsonl: wrote=1836250, skipped_empty=0\n",
      "[INFO] Missing-taxonomy positions encoded as unk_k_id: 3094452\n",
      "[OK] out_dir = /home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_silva_sintax/dataset_full_top999\n",
      "[CHECK] No negative tax ids found in first 2000 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'out_dir': '/home/hernan_melmoth/Documents/phd_work/Microbeatlas_preprocess_training/level_97/silva-138.2/incomplete_silva_sintax/dataset_full_top999',\n",
       " 'n_samples_total': 1836250,\n",
       " 'n_samples_written': 1836250,\n",
       " 'n_samples_empty': 0,\n",
       " 'n_otus_pre': 99335,\n",
       " 'n_otus_kept': 62200,\n",
       " 'n_tax_nodes': 6929,\n",
       " 'n_otus_missing_taxonomy': 3223,\n",
       " 'n_positions_missing_taxonomy': 3094452,\n",
       " 'unk_kingdom_id': 6928}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Load counts (big dict-of-dicts)\n",
    "with open(CFG[\"filtered_counts_path\"], \"r\") as f:\n",
    "    counts_by_sample = json.load(f)\n",
    "\n",
    "# 2) Run builder\n",
    "stats = build_dataset_full_corpus(\n",
    "    counts_by_sample=counts_by_sample,\n",
    "    pred_df=pred_df,\n",
    "    out_dir=OUT_DIR,\n",
    "    keep_fraction=CFG[\"keep_fraction\"],\n",
    ")\n",
    "\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a91fdc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: sample ids align with otu_vocab and taxonomy_vocab.\n",
      "Example sample_id: SRR4892887.SRS1780364\n",
      "First 10 OTU ids: [3, 22, 516, 31, 92, 151, 24, 11, 185, 171]\n",
      "First 10 TAXA ids: [1197, 1197, 1197, 1040, 1197, 1040, 1197, 1197, 2521, 2947]\n"
     ]
    }
   ],
   "source": [
    "otu_vocab = json.load(open(os.path.join(OUT_DIR, \"otu_vocab.json\")))\n",
    "tax_vocab = json.load(open(os.path.join(OUT_DIR, \"taxonomy_vocab.json\")))\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"samples.jsonl\")) as f:\n",
    "    first = json.loads(next(f))\n",
    "\n",
    "assert len(first[\"otus\"]) == len(first[\"taxa\"])\n",
    "assert max(first[\"otus\"]) < len(otu_vocab)\n",
    "assert max(first[\"taxa\"]) < len(tax_vocab)\n",
    "\n",
    "print(\"OK: sample ids align with otu_vocab and taxonomy_vocab.\")\n",
    "print(\"Example sample_id:\", first[\"sample_id\"])\n",
    "print(\"First 10 OTU ids:\", first[\"otus\"][:10])\n",
    "print(\"First 10 TAXA ids:\", first[\"taxa\"][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8856ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_ontology_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
